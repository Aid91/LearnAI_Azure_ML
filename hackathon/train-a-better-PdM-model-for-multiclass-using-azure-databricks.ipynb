{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-sandbox\n",
    "# Extending PdM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this execise, we address the following concerns:\n",
    "  - **A different feature set**: We go back to the original telemetry data, and this time instead of computing features consisting of rolling means and standard deviations, we instead run PCA (principal component analysis) on the telemetry data and then run the K-means clustering algorithm on the PCs (principal components). This will give us a set of K clusters based on the telemetry data. Our hope is that some of the clusters will represent cases where one or more telemetries \"go off the charts\". So we can use the clusters (after one-hot-encoding it) as features into the classification model instead of the original telemetries or the rolling means and standard deviations. If we are successful, we can argue that we have found a more simple feature set for the model.\n",
    "  - **Multi-class classification**: We extend the problem of binary classification into multi-class classification. Recall that the PdM data flags failure *by component*, so we know which component of a machine failed at any time. In previous notebooks, we built binary classifiers for predicting failure for one component, but now we extend this to a all the components. Our model should be able to predict which component fails given the machine's telemetries, meta-data, and time elapsed since the last maintenance and failure (for each component)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Run the following cell to configure our \"classroom.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../includes/setup_env\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by reading the raw data which has the telemetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['machineID', 'datetime']\n",
    "keep_left = ['volt', 'rotate', 'pressure', 'vibration']\n",
    "df_raw = spark.read.parquet(\"dbfs:/FileStore/tables/raw\").select(*keys + keep_left).cache()\n",
    "display(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also load the data we finished pre-processing in a prior Notebook, but we ignore the moving average and standard deviation features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = spark.read.parquet(\"dbfs:/FileStore/tables/processed\").cache()\n",
    "keep_right = ['age'] + [c for c in df_processed.columns if c.startswith('diff_') or c.startswith('y_')]\n",
    "df_processed = df_processed.select(*keys + keep_right).cache()\n",
    "display(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now join the two datasets into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.join(df_processed, on = keys, how = 'inner').cache()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform multi-class classification, we need to create a single column that encodes the four classes. We will call it `label` and let `label = 1` when `y_0 = 1` (component 1 fails), `label = 2` when `y_1 = 1` (component 2 fails), `label = 3` when `y_2 = 1` (component 3 fails), `label = 4` when `y_3 = 1` (component 4 fails), and `label = 0` when no component fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit, col\n",
    "\n",
    "df = df.withColumn(\"label\", lit(0))\n",
    "for i in range(4): # iterate over the four components\n",
    "    label = 'y_' + str(i) # name of target column (one per component)\n",
    "    find_labels = when((col(label) == 1), lit(i+1)).otherwise(col(\"label\"))\n",
    "    df = df.withColumn(\"label\", find_labels)\n",
    "\n",
    "df = df.drop(\"y_0\", \"y_1\", \"y_2\", \"y_3\").cache()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some summary statistics for the labels in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.groupBy(\"label\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by dividing the data into training and test sets. With time-series data, we usually divide the data based on a time cut-off and to avoid **leakage** we also put a gap (2 weeks in this case) between the training and test data. Another option we have is to sample every n-th row of the data. The data is collected hourly, and if we do not wish to use such a high frequency for modeling, we can sample every n-th row of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import DateType\n",
    "from pandas import datetime\n",
    "from pyspark.sql.functions import col, hour\n",
    "\n",
    "# we sample every nth row of the data using the `hour` function\n",
    "df_train = df.filter((col('datetime') < datetime(2015, 10, 1))) # & (hour(col('datetime')) % 3 == 0))\n",
    "df_test = df.filter(col('datetime') > datetime(2015, 10, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we don't have any null values in our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordCount = df_train.count()\n",
    "noNullsRecordCount = df_train.na.drop().count()\n",
    "\n",
    "print(\"We have {} records that contain null values.\".format(recordCount - noNullsRecordCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering using PCA and K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will learn to use two un-supervised learning algorithms, namely [PCA (principal component analysis)](https://en.wikipedia.org/wiki/Principal_component_analysis) and [K-means Clustering](https://www.google.com/search?client=firefox-b-1-ab&q=k-means+clustering) in order to expand on what we learned about feature engineering so far. Prior to running this exercise, we invite you to learn more about these algorithms if you are new to using them.\n",
    "\n",
    "- [Documentation for `KMeans`](https://spark.apache.org/docs/2.4.0/ml-clustering.html#k-means)\n",
    "- [Documentation for `PCA`](https://spark.apache.org/docs/2.4.0/ml-features.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, OneHotEncoderEstimator, MinMaxScaler\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "PCA_features = ['volt', 'rotate', 'pressure', 'vibration']\n",
    "diff_features = [c for c in df.columns if c.startswith('diff_')]\n",
    "\n",
    "stages = []\n",
    "# create a single vector feature from telemetry data\n",
    "stages.append(VectorAssembler(inputCols = PCA_features, outputCol = \"pca_raw_features\"))\n",
    "# extract principal components form the telemetry data (we chose k = 4 so there's no dimensionality reduction, just orthogonalization)\n",
    "stages.append(PCA(k = 4, inputCol = \"pca_raw_features\", outputCol = \"pca_features\"))\n",
    "# rescale principal components prior to running k-means\n",
    "stages.append(StandardScaler(inputCol = \"pca_features\", outputCol=\"scaled_pca_features\", withStd = True, withMean = False))\n",
    "# run k-means on rescaled principal components (we chose K = 3 to keep it simple for now)\n",
    "stages.append(KMeans(featuresCol = \"scaled_pca_features\", predictionCol = \"cluster\").setK(3).setSeed(1))\n",
    "# run one-hot encoding on cluster feature\n",
    "stages.append(OneHotEncoderEstimator(inputCols = [\"cluster\"], outputCols = [\"cluster_vec\"], dropLast = False))\n",
    "# combine all \"time-elapsed-since\" features into single vector\n",
    "stages.append(VectorAssembler(inputCols = diff_features, outputCol = \"diff_features\"))\n",
    "# rescale all \"time-elapsed-since\" features\n",
    "stages.append(MinMaxScaler(inputCol = \"diff_features\", outputCol=\"scaled_diff_features\"))\n",
    "# create one vector with all final features\n",
    "stages.append(VectorAssembler(inputCols = ['scaled_diff_features', 'age', 'cluster_vec'], outputCol = \"final_features\"))\n",
    "\n",
    "data_pipeline = Pipeline(stages = stages)\n",
    "print(data_pipeline.getStages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = data_pipeline.fit(df_train)\n",
    "\n",
    "df_kmeans = featurizer.transform(df_train).select(*keys + PCA_features + [\"label\", \"cluster\", \"final_features\"])\n",
    "display(df_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_kmeans.groupBy(\"cluster\").mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build some of the transformations we'll need in our pipeline.\n",
    "\n",
    "[Logistic Regression Docs](https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "lr = (LogisticRegression()\n",
    "     .setLabelCol(\"label\")\n",
    "     .setFeaturesCol(\"final_features\"))\n",
    "\n",
    "model_pipeline = Pipeline(stages = [lr])\n",
    "assert len(model_pipeline.getStages()) == 1 # make sure it's one stage only\n",
    "print(model_pipeline.getStages())\n",
    "\n",
    "lr_model = model_pipeline.fit(df_kmeans)\n",
    "\n",
    "df_pred = lr_model.transform(featurizer.transform(df_test)) # apply the model to our held-out test set\n",
    "display(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make sure that we use `MulticlassClassificationEvaluator`, not `BinaryClassificationEvaluator`. As we can see below, the evaluation metrics for the multi-class case are different from the binary case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import  MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "print(evaluator.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printEval(df, labelCol = \"label\"):\n",
    "  evaluator = MulticlassClassificationEvaluator()\n",
    "  evaluator.setLabelCol(labelCol)\n",
    "  \n",
    "  wrecall = evaluator.setMetricName(\"weightedPrecision\").evaluate(df)\n",
    "  wprecis = evaluator.setMetricName(\"weightedPrecision\").evaluate(df)\n",
    "  print(\"weighted recall: {}\\nweighted precision: {}\".format(wrecall, wprecis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printEval(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Random Forest Model\n",
    "\n",
    "Let's now compare this to what we get if we use a random forest with cross-validation instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = (RandomForestClassifier()\n",
    "      .setLabelCol(\"label\")\n",
    "      .setFeaturesCol(\"final_features\")\n",
    "      .setSeed(27))\n",
    "\n",
    "# print(rf.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a single-stage pipeline for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "model_pipeline = Pipeline(stages = [rf])\n",
    "\n",
    "model_pipeline.getStages()\n",
    "\n",
    "# model_pipeline.getStages()[0].extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a grid search to find the optimal combination of `maxDepth` and `numTrees` for the random forest, and use cross validation to evaluate the algorithm's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\\\n",
    "            .addGrid(rf.maxDepth, [5, 10, 20]) \\\n",
    "            .addGrid(rf.numTrees, [20, 50]) \\\n",
    "            .build())\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import  MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator().setMetricName(\"weightedPrecision\")\n",
    "\n",
    "cv = (CrossValidator()\n",
    "      .setEstimator(model_pipeline)\n",
    "      .setEvaluator(evaluator)\n",
    "      .setEstimatorParamMaps(paramGrid)\n",
    "      .setNumFolds(3)\n",
    "      .setSeed(27))\n",
    "\n",
    "cv_model = cv.fit(df_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the result of the grid search and the average (cross-validated) evaluation metric here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = list(zip(cv_model.getEstimatorParamMaps(), cv_model.avgMetrics))\n",
    "[(list(ll[i][0].values()), ll[i][1]) for i in range(len(ll))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "Finally, here's the final model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = cv_model.transform(featurizer.transform(df_test))\n",
    "\n",
    "printEval(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  }
 ],
 "metadata": {
  "name": "04c_hackathon_multiclass",
  "notebookId": 4079375216931215
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
