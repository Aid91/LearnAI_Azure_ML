{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Azure Monitor Alerts and Gates\n\nGates also allow us to query Azure monitor alerts. In this lab, we will begin by leveraging _Application Insights_ to create alerts. Azure Application Insights can alert you to changes in performance or usage metrics in your app. We will then monitor the alert rules in our deployment gates.\n\nA typical scenario is where a scoring service is deployed and the service is monitored for some time interval for before promoting it to production."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Application Insights\n\n1. In the portal, navigate to your _Azure Machine Learning Service Workspace_ page and select the `Application Insights` account.\n\n![applicationInsights](../images/applicationInsights.png)\n\n2. In the _Application Insights_ dashboard, select _Alerts_:\n\n![alerts](../images/alerts.png)\n\n3. Select _View classic alerts_ as shown below:\n\n![classicAlerts](../images/classicAlerts.png)\n\n4. Select _Add metric alert_:\n\n![addMetricAlert](../images/addMetricAlert.png)\n\n5. In the _Add Rule_ window, provide a name for the rule. You will find that the Criteria information is already populated. Select a metric to use in the rule. For example, you can use `Server Response Time`, `Availability` or `Failed Requests` along with _Condition_, _Threshold value_ and _Period_. A sample rule can be \"Server response time less than 3\".\n\n![addRule](../images/addRule.png)\n\n6. In the _Notify via_ section, you can list email addresses seperated by semicolons in `Notification email recipients` for getting notified.\n\n7. On success, you will be able to view the rules in _Alerts (classic)_ dashboard:\n\n![rules](../images/rules.png)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "### Query Azure Monitor Alerts\n\nEdit the release pipeline created from the previous lab. The release pipeline should have pre-deployment gates enabled. By now, you should have an idea of how to add gates to release stages. Refer to previous lab for guidance (if you need).\n\n1. Select `Pre-deployment conditions` in the stages and under `Gates`, select _+ Add_ followed by `Query Azure Monitor alerts`.\n\n![gatesAlerts](../images/gatesAlerts.png)\n\nFill out all the mandatory fields with _Query Azure Monitor Alerts_ and save.\n\n| Name   |      Description      |\n|----------|-------------|\n| _Display Name_ |  A mandatory name for gates related to Query Azure Monitor Alerts |\n| _Azure Subscription_ |    Azure Resource Manager subscription to monitor   |\n| _Resource group_ |    Name of resource group to monitor   |\n| _Resource type_ |    Name of resource type to monitor. In this lab, we will use Application Insights   |\n| _Resource name_ |    Name of resource name to monitor. In this lab, we will use the name of the Application Insights resource   |\n| _Alert rules_ |    List of Azure alert rules to monitor. Select the alert rule created in the previous section.   |\n\n2. Navigate to Releases and open the release pipeline saved. Deploy/Redeploy:\n\n![deploy](../images/deploy.png)\n\n3. Select _Logs_ of _Pre-deployment gates_ to verify if all gates succeeded. \n\n![gatesSuccess](../images/gatesSuccess.png)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "### Exercise\n\n1. Can you add another alert rule related to _availability_ to the gate?\n\n2. (Optional) The _Query Azure Monitor alerts_ passed in this scenario because there wasnt any data for response time. However, you can create a sccoring service using AKS and pass more scoring data and and then redoploy to see if the additional data makes a difference. Run the below script (after replacing <scoring_url> and <key>) locally to obtain the predictions. You can also change input_j to obtain different predictions.\n\n````python\n    import requests\n    import json\n\n    input_j = [[1.92168882e+02, 5.82427351e+02, 2.09748253e+02, 4.32529303e+01, 1.52377597e+01, 5.37307613e+01, 1.15729573e+01, 4.27624778e+00, 1.68042813e+02, 4.61654301e+02, 1.03138200e+02, 4.08555785e+01, 1.80809993e+01, 4.85402042e+01, 1.09373285e+01, 4.18269355e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.07200000e+03, 5.64000000e+02, 2.22900000e+03, 9.84000000e+02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.03000000e+02, 6.63000000e+02, 3.18300000e+03, 3.03000000e+02, 5.34300000e+03, 4.26300000e+03, 6.88200000e+03, 1.02300000e+03, 1.80000000e+01]]\n\n    data = json.dumps({'data': input_j})\n    test_sample = bytes(data, encoding = 'utf8')\n\n    url = '<scoring_url>'\n    api_key = '<key>' \n    headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n\n    resp = requests.post(url, test_sample, headers=headers)\n    print(resp.text)\n````"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}