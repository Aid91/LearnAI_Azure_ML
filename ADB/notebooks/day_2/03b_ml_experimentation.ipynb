{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../includes/setup_env\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run a training experiment and use the Azure ML SDK to save it to our AML Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our Azure ML Workspace first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Workspace class and check the azureml SDK version\n",
    "from azureml.core import Workspace\n",
    "\n",
    "config_path = '/dbfs/tmp/'\n",
    "\n",
    "ws = Workspace.from_config(path=os.path.join(config_path, 'aml_config','config.json'))\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"dbfs:/FileStore/tables/preprocessed\").cache()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import DateType\n",
    "from pandas import datetime\n",
    "from pyspark.sql.functions import col, hour\n",
    "\n",
    "# we sample every nth row of the data using the `hour` function\n",
    "df_train = df.filter((col('datetime') < datetime(2015, 10, 1))) # & (hour(col('datetime')) % 3 == 0))\n",
    "df_test = df.filter(col('datetime') > datetime(2015, 10, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(\"y_1\",\"y_2\",\"y_3\",\"datetime\", \"machineID\")\n",
    "df_train = df_train.withColumnRenamed(\"y_0\", \"error\")\n",
    "df_train.cache()\n",
    "\n",
    "df_test = df_test.drop(\"y_1\",\"y_2\",\"y_3\",\"datetime\", \"machineID\")\n",
    "df_test = df_test.withColumnRenamed(\"y_0\", \"error\")\n",
    "df_test.cache()\n",
    "\n",
    "print(\"train: ({}, {})\".format(df_train.count(), len(df_train.columns)))\n",
    "print(\"test: ({}, {})\".format(df_test.count(), len(df_test.columns)))\n",
    "\n",
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to run the experiment. To do so we load the root experiment and call the `start_logging` method. We then invoke each iteration of the experiment using the `run` and tell it which metrics to log. Examine the code below and see it all happening in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.run import Run\n",
    "from azureml.core.experiment import Experiment\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "model_name = \"PdM_logistic_regression.mml\"\n",
    "model_dbfs = os.path.join(\"/dbfs\", model_name)\n",
    "run_history_name = 'spark-ml-notebook'\n",
    "\n",
    "# start a training run by defining an experiment\n",
    "myexperiment = Experiment(ws, \"AI_Airlft\")\n",
    "root_run = myexperiment.start_logging()\n",
    "\n",
    "# Regularization Rates - \n",
    "regs = [0.0001, 0.001, 0.01, 0.1]\n",
    " \n",
    "# try a bunch of regularization rate in a Logistic Regression model\n",
    "for reg in regs:\n",
    "    print(\"Regularization rate: {}\".format(reg))\n",
    "    # create a bunch of child runs\n",
    "    with root_run.child_run(\"reg-\" + str(reg)) as run:\n",
    "        # create a new Logistic Regression model.\n",
    "        lr = (LogisticRegression(regParam=reg)\n",
    "              .setLabelCol(\"error\")\n",
    "              .setFeaturesCol(\"norm_features\"))\n",
    "        \n",
    "        # put together the pipeline\n",
    "        pipe = Pipeline(stages=[lr])\n",
    "\n",
    "        # train the model\n",
    "        model_p = pipe.fit(df_train)\n",
    "        \n",
    "        # make prediction\n",
    "        pred = model_p.transform(df_test)\n",
    "        \n",
    "        # evaluate. note only 2 metrics are supported out of the box by Spark ML.\n",
    "        bce = (BinaryClassificationEvaluator()\n",
    "               .setLabelCol(\"error\")\n",
    "               .setRawPredictionCol('rawPrediction'))\n",
    "               \n",
    "        au_roc = bce.setMetricName('areaUnderROC').evaluate(pred)\n",
    "        au_prc = bce.setMetricName('areaUnderPR').evaluate(pred)\n",
    "\n",
    "        print(\"Area under ROC: {}\".format(au_roc))\n",
    "        print(\"Area Under PR: {}\".format(au_prc))\n",
    "      \n",
    "        # log reg, au_roc, au_prc and feature names in run history\n",
    "        run.log(\"reg\", reg)\n",
    "        run.log(\"au_roc\", au_roc)\n",
    "        run.log(\"au_prc\", au_prc)\n",
    "        run.log_list(\"columns\", df_train.columns)\n",
    "\n",
    "        # save model\n",
    "        model_p.write().overwrite().save(model_name)\n",
    "        \n",
    "        # upload the serialized model into run history record\n",
    "        mdl, ext = model_name.split(\".\")\n",
    "        model_zip = mdl + \".zip\"\n",
    "        shutil.make_archive(mdl, 'zip', model_dbfs)\n",
    "        run.upload_file(\"outputs/\" + model_name, model_zip)        \n",
    "        #run.upload_file(\"outputs/\" + model_name, path_or_stream = model_dbfs) #cannot deal with folders\n",
    "\n",
    "        # now delete the serialized model from local folder since it is already uploaded to run history \n",
    "        shutil.rmtree(model_dbfs)\n",
    "        os.remove(model_zip)\n",
    "        \n",
    "# Declare run completed\n",
    "root_run.complete()\n",
    "root_run_id = root_run.id\n",
    "print (\"run id:\", root_run.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all run metrics from run history into a dictionary object\n",
    "child_runs = {}\n",
    "\n",
    "for r in root_run.get_children():\n",
    "    child_runs[r.id] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now select the best model based on the metric we choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = root_run.get_metrics(recursive = True)\n",
    "best_run_id = max(metrics, key = lambda k: metrics[k]['au_roc'])\n",
    "best_run = child_runs[best_run_id]\n",
    "print('Best run is:', best_run_id)\n",
    "print('Metrics:', metrics[best_run_id]['au_roc'], metrics[best_run_id]['reg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the best model on disk for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model from the best run to a local folder\n",
    "best_model_file_name = \"best_model.zip\"\n",
    "best_run.download_file(name = 'outputs/' + model_name, output_file_path = best_model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the best model we selected earlier and use it to evaluate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unzip the model to dbfs (as load() seems to require that) and load it\n",
    "if os.path.isfile(model_dbfs) or os.path.isdir(model_dbfs):\n",
    "    shutil.rmtree(model_dbfs)\n",
    "shutil.unpack_archive(best_model_file_name, model_dbfs)\n",
    "\n",
    "model_p_best = PipelineModel.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "df_pred = model_p_best.transform(df_test)\n",
    "display(df_pred.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_select = df_pred.orderBy(F.desc('prediction')).limit(5)\n",
    "df_select = df_pred.union(df_pred.orderBy(F.asc('prediction')).limit(5))\n",
    "\n",
    "display(df_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate. note only 2 metrics are supported out of the box by Spark ML.\n",
    "bce = (BinaryClassificationEvaluator()\n",
    "               .setLabelCol(\"error\")\n",
    "               .setRawPredictionCol('rawPrediction'))\n",
    "au_roc = bce.setMetricName('areaUnderROC').evaluate(df_pred)\n",
    "au_prc = bce.setMetricName('areaUnderPR').evaluate(df_pred)\n",
    "\n",
    "print(\"Area under ROC: {}\".format(au_roc))\n",
    "print(\"Area Under PR: {}\".format(au_prc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_name[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: by default the model is saved to and loaded from /dbfs/ instead of cwd!\n",
    "model_p_best.write().overwrite().save(model_name[:-4])\n",
    "print(\"saved model to {}\".format(model_dbfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "ls -la /dbfs/PdM_logistic_regression/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "pasha"
   },
   {
    "name": "wamartin"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "name": "03.Build_model_runHistory",
  "notebookId": 4057188818416674
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
