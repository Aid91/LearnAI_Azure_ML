{"cells":[{"cell_type":"markdown","source":["Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."],"metadata":{}},{"cell_type":"code","source":["%run \"../includes/setup_env\""],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["#Model Building"],"metadata":{}},{"cell_type":"markdown","source":["We now run a training experiment and use the Azure ML SDK to save it to our AML Workspace."],"metadata":{}},{"cell_type":"code","source":["import os\nimport pprint\nimport numpy as np\n\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["import azureml.core\n\n# Check core SDK version number\nprint(\"SDK version:\", azureml.core.VERSION)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Let's load our Azure ML Workspace first:"],"metadata":{}},{"cell_type":"code","source":["# import the Workspace class and check the azureml SDK version\nfrom azureml.core import Workspace\n\nconfig_path = '/dbfs/tmp/'\n\nws = Workspace.from_config(path=os.path.join(config_path, 'aml_config','config.json'))\nprint('Workspace name: ' + ws.name, \n      'Azure region: ' + ws.location, \n      'Resource group: ' + ws.resource_group, sep = '\\n')"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["df = spark.read.parquet(\"dbfs:/FileStore/tables/preprocessed\").cache()\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# from pyspark.sql.types import DateType\nfrom pandas import datetime\nfrom pyspark.sql.functions import col, hour\n\n# we sample every nth row of the data using the `hour` function\ndf_train = df.filter((col('datetime') < datetime(2015, 10, 1))) # & (hour(col('datetime')) % 3 == 0))\ndf_test = df.filter(col('datetime') > datetime(2015, 10, 15))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["df_train = df_train.drop(\"y_1\",\"y_2\",\"y_3\",\"datetime\", \"machineID\")\ndf_train = df_train.withColumnRenamed(\"y_0\", \"error\")\ndf_train.cache()\n\ndf_test = df_test.drop(\"y_1\",\"y_2\",\"y_3\",\"datetime\", \"machineID\")\ndf_test = df_test.withColumnRenamed(\"y_0\", \"error\")\ndf_test.cache()\n\nprint(\"train: ({}, {})\".format(df_train.count(), len(df_train.columns)))\nprint(\"test: ({}, {})\".format(df_test.count(), len(df_test.columns)))\n\ndf_train.printSchema()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["#Define Model"],"metadata":{}},{"cell_type":"markdown","source":["It is time to run the experiment. To do so we load the root experiment and call the `start_logging` method. We then invoke each iteration of the experiment using the `run` and tell it which metrics to log. Examine the code below and see it all happening in action."],"metadata":{}},{"cell_type":"code","source":["from azureml.core.run import Run\nfrom azureml.core.experiment import Experiment\nimport numpy as np\nimport os\nimport shutil\n\nmodel_name = \"PdM_logistic_regression.mml\"\nmodel_dbfs = os.path.join(\"/dbfs\", model_name)\nrun_history_name = 'spark-ml-notebook'\n\n# start a training run by defining an experiment\nmyexperiment = Experiment(ws, \"AI_Airlft\")\nroot_run = myexperiment.start_logging()\n\n# Regularization Rates - \nregs = [0.0001, 0.001, 0.01, 0.1]\n \n# try a bunch of regularization rate in a Logistic Regression model\nfor reg in regs:\n    print(\"Regularization rate: {}\".format(reg))\n    # create a bunch of child runs\n    with root_run.child_run(\"reg-\" + str(reg)) as run:\n        # create a new Logistic Regression model.\n        lr = (LogisticRegression(regParam=reg)\n              .setLabelCol(\"error\")\n              .setFeaturesCol(\"norm_features\"))\n        \n        # put together the pipeline\n        pipe = Pipeline(stages=[lr])\n\n        # train the model\n        model_p = pipe.fit(df_train)\n        \n        # make prediction\n        pred = model_p.transform(df_test)\n        \n        # evaluate. note only 2 metrics are supported out of the box by Spark ML.\n        bce = (BinaryClassificationEvaluator()\n               .setLabelCol(\"error\")\n               .setRawPredictionCol('rawPrediction'))\n               \n        au_roc = bce.setMetricName('areaUnderROC').evaluate(pred)\n        au_prc = bce.setMetricName('areaUnderPR').evaluate(pred)\n\n        print(\"Area under ROC: {}\".format(au_roc))\n        print(\"Area Under PR: {}\".format(au_prc))\n      \n        # log reg, au_roc, au_prc and feature names in run history\n        run.log(\"reg\", reg)\n        run.log(\"au_roc\", au_roc)\n        run.log(\"au_prc\", au_prc)\n        run.log_list(\"columns\", df_train.columns)\n\n        # save model\n        model_p.write().overwrite().save(model_name)\n        \n        # upload the serialized model into run history record\n        mdl, ext = model_name.split(\".\")\n        model_zip = mdl + \".zip\"\n        shutil.make_archive(mdl, 'zip', model_dbfs)\n        run.upload_file(\"outputs/\" + model_name, model_zip)        \n        #run.upload_file(\"outputs/\" + model_name, path_or_stream = model_dbfs) #cannot deal with folders\n\n        # now delete the serialized model from local folder since it is already uploaded to run history \n        shutil.rmtree(model_dbfs)\n        os.remove(model_zip)\n        \n# Declare run completed\nroot_run.complete()\nroot_run_id = root_run.id\nprint (\"run id:\", root_run.id)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# load all run metrics from run history into a dictionary object\nchild_runs = {}\n\nfor r in root_run.get_children():\n    child_runs[r.id] = r"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["We can now select the best model based on the metric we choose."],"metadata":{}},{"cell_type":"code","source":["metrics = root_run.get_metrics(recursive = True)\nbest_run_id = max(metrics, key = lambda k: metrics[k]['au_roc'])\nbest_run = child_runs[best_run_id]\nprint('Best run is:', best_run_id)\nprint('Metrics:', metrics[best_run_id]['au_roc'], metrics[best_run_id]['reg'])"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["We save the best model on disk for future use."],"metadata":{}},{"cell_type":"code","source":["# download the model from the best run to a local folder\nbest_model_file_name = \"best_model.zip\"\nbest_run.download_file(name = 'outputs/' + model_name, output_file_path = best_model_file_name)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["#Model Evaluation"],"metadata":{}},{"cell_type":"markdown","source":["We can load the best model we selected earlier and use it to evaluate its accuracy."],"metadata":{}},{"cell_type":"code","source":["## unzip the model to dbfs (as load() seems to require that) and load it\nif os.path.isfile(model_dbfs) or os.path.isdir(model_dbfs):\n    shutil.rmtree(model_dbfs)\nshutil.unpack_archive(best_model_file_name, model_dbfs)\n\nmodel_p_best = PipelineModel.load(model_name)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# make prediction\ndf_pred = model_p_best.transform(df_test)\ndisplay(df_pred.limit(5))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["import pyspark.sql.functions as F\n\ndf_select = df_pred.orderBy(F.desc('prediction')).limit(5)\ndf_select = df_pred.union(df_pred.orderBy(F.asc('prediction')).limit(5))\n\ndisplay(df_select)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# evaluate. note only 2 metrics are supported out of the box by Spark ML.\nbce = (BinaryClassificationEvaluator()\n               .setLabelCol(\"error\")\n               .setRawPredictionCol('rawPrediction'))\nau_roc = bce.setMetricName('areaUnderROC').evaluate(df_pred)\nau_prc = bce.setMetricName('areaUnderPR').evaluate(df_pred)\n\nprint(\"Area under ROC: {}\".format(au_roc))\nprint(\"Area Under PR: {}\".format(au_prc))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["#Model Persistence"],"metadata":{}},{"cell_type":"code","source":["print(model_name[:-4])"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["## NOTE: by default the model is saved to and loaded from /dbfs/ instead of cwd!\nmodel_p_best.write().overwrite().save(model_name[:-4])\nprint(\"saved model to {}\".format(model_dbfs))"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["%sh\n\nls -la /dbfs/PdM_logistic_regression/*"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["dbutils.notebook.exit(\"success\")"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."],"metadata":{}}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.0","nbconvert_exporter":"python","file_extension":".py"},"name":"03.Build_model_runHistory","notebookId":3771102850753995,"kernelspec":{"display_name":"Python 3.6","language":"python","name":"python36"},"authors":[{"name":"pasha"},{"name":"wamartin"}]},"nbformat":4,"nbformat_minor":0}
