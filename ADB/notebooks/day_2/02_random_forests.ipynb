{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Random Forests\n",
    "\n",
    "In this lab, we are going to use the same dataset as in the last lab, but we are going to use a Random Forest instead of a single decision tree. \n",
    "\n",
    "We will also use a parameter grid and cross-validation to perform hyperparameter tuning, as well as export our final model.\n",
    "\n",
    "The code below is taken from the last lab to set up our data transformations.\n",
    "\n",
    "[Random Forest Scala Docs](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.RandomForestRegressor)\n",
    "\n",
    "[Random Forest Python Docs](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor)\n",
    "\n",
    "[Spark ML Guide](https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../includes/setup_env\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and cache the data once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"dbfs:/FileStore/tables/preprocessed\").cache()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import DateType\n",
    "from pandas import datetime\n",
    "from pyspark.sql.functions import col, hour\n",
    "\n",
    "# we sample every nth row of the data using the `hour` function\n",
    "df_train = df.filter((col('datetime') < datetime(2015, 10, 1))) # & (hour(col('datetime')) % 3 == 0))\n",
    "df_test = df.filter(col('datetime') > datetime(2015, 10, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(\"y_1\",\"y_2\",\"y_3\",\"datetime\", \"machineID\")\n",
    "df_train = df_train.withColumnRenamed(\"y_0\", \"error\")\n",
    "df_train.cache()\n",
    "\n",
    "df_test = df_test.drop(\"y_1\",\"y_2\",\"y_3\",\"datetime\", \"machineID\")\n",
    "df_test = df_test.withColumnRenamed(\"y_0\", \"error\")\n",
    "df_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Random Forests\n",
    "\n",
    "Random forests and ensembles of decision trees are more powerful than a single decision tree alone. \n",
    "\n",
    "Let's take a look at all the hyperparameters we could change in [RandomForestClassifier](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestClassifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = (RandomForestClassifier()\n",
    "      .setLabelCol(\"error\")\n",
    "      .setFeaturesCol(\"norm_features\")\n",
    "      .setSeed(27))\n",
    "\n",
    "print(rf.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on lab\n",
    "Try changing the values of `numTrees` and `maxDepth` to any values you like\n",
    "\n",
    "HINT: Take a look at the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximize this cell (click the + button on the right) to see the solution:\n",
    "  \n",
    "rf.numTrees = 100\n",
    "rf.maxDepth = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Pipeline\n",
    "\n",
    "Now that we have all of the feature transformations and estimators set up, let's put all of the stages together in the pipline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages = [rf])\n",
    "\n",
    "pipeline.getStages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to look at what parameter each stage in the pipeline takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.getStages()[0].extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) ParamGrid\n",
    "\n",
    "There are a lot of hyperparamaters we could tune, and it would take a long time to manually configure.\n",
    "\n",
    "Instead of a manual (ad-hoc) approach, let's use Spark's [ParamGridBuilder](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder) to find the optimal hyperparameters in a more systematic approach.\n",
    "\n",
    "In this example notebook, we keep these trees shallow and use a relatively small number of trees. Let's define a grid of hyperparameters to test:\n",
    "  - maxDepth: max depth of each decision tree in the RF ensemble (Use the values `2, 5, 10`)\n",
    "  - numTrees: number of trees in each RF ensemble (Use the values `10, 50`)\n",
    "\n",
    "`addGrid()` accepts the name of the parameter (e.g. `rf.maxDepth`), and a list of the possible values (e.g. `[2, 5, 10]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximize this cell (click the + button on the right) to see the solution:\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\\\n",
    "            .addGrid(rf.maxDepth, [2, 5, 10]) \\\n",
    "            .addGrid(rf.numTrees, [10, 50]) \\\n",
    "            .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Cross Validation\n",
    "\n",
    "We are also going to use 3-fold cross validation to identify the optimal maxDepth and numTrees combination.\n",
    "\n",
    "![crossValidation](https://files.training.databricks.com/images/301/CrossValidation.png)\n",
    "\n",
    "With 3-fold cross-validation, we train on 2/3 of the data, and evaluate with the remaining (held-out) 1/3. We repeat this process 3 times, so each fold gets the chance to act as the validation set. We then average the results of the three rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass in the `estimator` (pipeline), `evaluator`, and `estimatorParamMaps` to [CrossValidator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) so that it knows:\n",
    "- Which model to use\n",
    "- How to evaluate the model\n",
    "- What hyperparamters to set for the model\n",
    "\n",
    "We can also set the number of folds we want to split our data into (3), as well as setting a seed so we all have the same split in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import  BinaryClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "evaluator = (BinaryClassificationEvaluator()\n",
    "             .setLabelCol(\"error\")\n",
    "             .setRawPredictionCol(\"rawPrediction\")\n",
    "             .setMetric('accuracy'))\n",
    "\n",
    "cv = (CrossValidator()\n",
    "      .setEstimator(pipeline)\n",
    "      .setEvaluator(evaluator)\n",
    "      .setEstimatorParamMaps(paramGrid)\n",
    "      .setNumFolds(3)\n",
    "      .setSeed(27))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on lab\n",
    "\n",
    "Depending on your Runtime version, you may have received an error message when running the previous cell. Can you use the the pyspark documentation to fix the code above to make it run in your Runtime version?\n",
    "\n",
    "Start with:\n",
    "> help(BinaryClassificationEvaluator)\n",
    "\n",
    "### End of lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the model with the best hyperparameter configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(cvModel.getEstimatorParamMaps(), cvModel.avgMetrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Save Model\n",
    "\n",
    "Let's save our model by writing it out. \n",
    "\n",
    "**NOTE:** We cannot save a pipeline model with a cross-validation step in Python. Instead, we have to save the best pipeline model itself.\n",
    "\n",
    "Also, there is no `overwrite` method. Our only alternative is to recursively delete the existing directory if we want to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/tmp/random_forest_pipeline\"\n",
    "\n",
    "modelPath = userhome + path\n",
    "dbutils.fs.rm(modelPath, recurse=True)\n",
    "\n",
    "cvModel.bestModel.save(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the saved model back in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "savedPipelineModel = PipelineModel.load(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the trained model to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = savedPipelineModel.transform(df_test)\n",
    "display(df_pred.select(\"error\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Evaluate\n",
    "\n",
    "Let's see how well we did on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printEval(df, labelCol = \"error\", rawPredictionCol = \"prediction\"):\n",
    "  evaluator = BinaryClassificationEvaluator()\n",
    "  evaluator.setLabelCol(labelCol)\n",
    "  evaluator.setRawPredictionCol(rawPredictionCol)\n",
    "\n",
    "  auroc = evaluator.setMetricName(\"areaUnderROC\").evaluate(df)\n",
    "  print(\"AUROC: {}\".format(auroc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printEval(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving our model\n",
    "\n",
    "You are not done yet!  There are several ways we could further improve our model:\n",
    "* **Expert knowledge** \n",
    "* **Better tuning** \n",
    "* **Feature engineering**\n",
    "\n",
    "As an exercise: Replace the Random Forest code with a Gradient Boosted tree, and vary the number of trees and depth of the trees. What do you find?\n",
    "\n",
    "*Good luck!*"
   ]
  }
 ],
 "metadata": {
  "name": "02_random_forests",
  "notebookId": 4057188818416634
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
