{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please ensure you have run all previous notebooks in sequence before running this.\n",
    "\n",
    "Please Register Azure Container Instance(ACI) using Azure Portal: https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-supported-services#portal in your subscription before using the SDK to deploy your ML model to ACI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "import azureml.core\n",
    "import os\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "config_path = '/dbfs/tmp/'\n",
    "\n",
    "#'''\n",
    "ws = Workspace.from_config(path=os.path.join(config_path, 'aml_config', 'config.json'))\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: service deployment always gets the model from the current working dir.\n",
    "model_name = \"PdM_logistic_regression.mml\" # \n",
    "model_name_dbfs = os.path.join(\"/dbfs\", model_name)\n",
    "\n",
    "print(\"copy model from dbfs to local\")\n",
    "model_local = \"file:\" + os.getcwd() + \"/\" + model_name\n",
    "dbutils.fs.cp(model_name, model_local, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the model\n",
    "from azureml.core.model import Model\n",
    "mymodel = Model.register(model_path = model_name, # this points to a local file\n",
    "                       model_name = model_name, # this is the name the model is registered as, am using same name for both path and name.                 \n",
    "                       description = \"ADB trained model by an amazing data scientist\",\n",
    "                       workspace = ws)\n",
    "\n",
    "print(mymodel.name, mymodel.description, mymodel.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting your data to and from JSON\n",
    "\n",
    "The most common way to interact with a webservice is using a [REST](https://en.wikipedia.org/wiki/Representational_state_transfer) API, sending and receiving [JSON](https://en.wikipedia.org/wiki/JSON) data.  \n",
    "\n",
    "We therefore need to convert our dataframe to JSON to send it to the webservice, and the webservice has to then convert it back into a dataframe so that we can use our pyspark model to score the data.\n",
    "\n",
    "Very often this is straightforward, because json can interpret the schema of our data correctly. However, this is not always the case.  Our usecase is an example where we need to help spark, by explicitly providing the schema when converting the JSON data back to a dataframe.\n",
    "\n",
    "Let's start with an example to illustrate that.\n",
    "\n",
    "  **Note**: Explicitly providing the schema of data is generally good practice, because it can speed up reading data and avoids surprises.  This is not only try when working with spark, but also e.g. in *R*  or *scikit-learn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"dbfs:/FileStore/tables/preprocessed\").cache()\n",
    "display(df)\n",
    "\n",
    "# from pyspark.sql.types import DateType\n",
    "from pandas import datetime\n",
    "from pyspark.sql.functions import col, hour\n",
    "\n",
    "# we sample every nth row of the data using the `hour` function\n",
    "df_train = df.filter((col('datetime') < datetime(2015, 10, 1)))\n",
    "df_test = df.filter(col('datetime') > datetime(2015, 10, 15)).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_path = \"TestData\"\n",
    "\n",
    "# test_data_path_dbfs = os.path.join(\"/dbfs\", test_data_path)\n",
    "\n",
    "# df_test = spark.read.parquet(test_data_path).limit(5)\n",
    "\n",
    "display(df_test.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "test_json = json.dumps(df_test.toJSON().collect())\n",
    "\n",
    "print(test_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = json.loads(test_json)\n",
    "input_rdd = sc.parallelize(input_list)\n",
    "input_df = spark.read.json(input_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see whether the data look as expected after the rountrip though JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is the schema of the original data frame:\")\n",
    "df_test.printSchema()\n",
    "\n",
    "print(\"This is the schema of our data frame after converting it to/from JSON:\")\n",
    "input_df.printSchema()\n",
    "\n",
    "try:\n",
    "  assert(df_test.schema == input_df.schema)\n",
    "except AssertionError:\n",
    "  print(\"Sadly, the schemas of the two data frames are not the same.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on Lab\n",
    "\n",
    "Help spark by explicitly providing the schema when reading the JSON data.\n",
    "\n",
    "This requires several parts:\n",
    "1. Identify the schema of the original data\n",
    "1. Create a schema definition that spark can use when reading the JSON data\n",
    "1. Tell spark to use that schema definition when reading the JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's identify the schma\n",
    "df_test.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. It looks like:\n",
    "- `norm_features` are encoded as a `VectorUDT`\n",
    "- `error` is encoded as `IntegerType`\n",
    "\n",
    "The schema definition further depends on the classes `StructType` and `StructField`.\n",
    "\n",
    "Try to find where those are defined using the pyspark API, and add the import statements at the top of the next cell. Hint, you need two lines of code.\n",
    "\n",
    "Use the search function of the pyspark API [documentation](https://spark.apache.org/docs/latest/api/python/index.html) to find the location of most of the definitions of these classes. Unfortunately, `VectorUDT` is a little bit harder to find, and will require some finesse on your side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.<...> import <...>\n",
    "#from pyspark.<...> import <...>\n",
    "\n",
    "myschema = StructType([\n",
    "                      StructField(\"norm_features\",VectorUDT()),\n",
    "                      StructField(\"error\",IntegerType())\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, IntegerType\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "myschema = StructType([\n",
    "                      StructField(\"norm_features\",VectorUDT()),\n",
    "                      StructField(\"error\",IntegerType())\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you were able to define the schema, tell spark to use it when reading the JSON data.\n",
    "\n",
    "Instead of simply writing `spark.read.json(input_rdd)`, tell spark to use your schema while reading the data.\n",
    "\n",
    "Use this [documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=spark%20read%20schema#pyspark.sql.DataFrameReader.schema) for some hint on how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = json.loads(test_json)\n",
    "input_rdd = sc.parallelize(input_list)\n",
    "# todo: modify the next line \n",
    "input_df = spark.read.json(input_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = json.loads(test_json)\n",
    "input_rdd = sc.parallelize(input_list)\n",
    "# todo: modify the next line \n",
    "input_df = spark.read.schema(myschema).json(input_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see whether you were successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is the schema of the original data frame:\")\n",
    "df_test.printSchema()\n",
    "\n",
    "print(\"This is the schema of our data frame after converting it to/from JSON:\")\n",
    "input_df.printSchema()\n",
    "\n",
    "try:\n",
    "  assert(df_test.schema == input_df.schema)\n",
    "  print(\"You did it!\")\n",
    "except AssertionError:\n",
    "  print(\"Sadly, the schemas of the two data frames are not the same.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of lab\n",
    "\n",
    "## Create a score file\n",
    "\n",
    "The next step of creating a web service is to define a score script that defines what the webservice does.\n",
    "\n",
    "A typical score script has two methods defined:\n",
    "- `init` is executed once, when the webservice is started\n",
    "- `run` is executed everytime a user is interacting with the webservice to score data\n",
    "\n",
    "Look at this score script below, can you see where we made the changes that are related to explicitly providing the schema when reading JSON data?\n",
    "\n",
    "There are several places:\n",
    "1. Importing the modules for defining the schema\n",
    "1. Defining a global variable for holding the schema\n",
    "1. Defining the schema\n",
    "1. Using the schema when reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sparkml = \"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "def init():\n",
    "    # One-time initialization of PySpark and predictive model\n",
    "    import pyspark\n",
    "    from azureml.core.model import Model\n",
    "    from pyspark.ml import PipelineModel\n",
    "    from pyspark.sql.types import StructField, StructType, IntegerType\n",
    "    from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "    global trainedModel\n",
    "    global spark\n",
    "    global schema\n",
    "    \n",
    "    spark = pyspark.sql.SparkSession.builder.appName(\"ADB and AML notebook by an amazing data scientist\").getOrCreate()\n",
    "    model_name = \"{model_name}\" #interpolated\n",
    "    model_path = Model.get_model_path(model_name)\n",
    "    trainedModel = PipelineModel.load(model_path)\n",
    "    \n",
    "    schema = StructType([StructField(\"norm_features\",VectorUDT()), StructField(\"error\",IntegerType())])\n",
    "    \n",
    "def run(input_json):\n",
    "    if isinstance(trainedModel, Exception):\n",
    "        return json.dumps({{\"trainedModel\":str(trainedModel)}})\n",
    "      \n",
    "    try:\n",
    "        sc = spark.sparkContext\n",
    "        input_list = json.loads(input_json)\n",
    "        input_rdd = sc.parallelize(input_list)\n",
    "        input_df = spark.read.schema(schema).json(input_rdd)\n",
    "        \n",
    "        # Compute prediction\n",
    "        prediction = trainedModel.transform(input_df)\n",
    "        #result = prediction.first().prediction\n",
    "        predictions = prediction.collect()\n",
    "\n",
    "        #Get each scored result\n",
    "        preds = [str(x['prediction']) for x in predictions]\n",
    "        result = \",\".join(preds)\n",
    "        # you can return any data type as long as it is JSON-serializable\n",
    "        return json.dumps({{\"result\":result}})        \n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return json.dumps({{\"error\":result}})\n",
    "    \n",
    "\"\"\".format(model_name=model_name)\n",
    "\n",
    "exec(score_sparkml)\n",
    "\n",
    "with open(\"score_sparkml.py\", \"w\") as file:\n",
    "    file.write(score_sparkml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a webservice requires creating a docker container in which to run our score script. \n",
    "\n",
    "This can all be done with the python AML sdk. \n",
    "\n",
    "First we create a conda environment, which makes sure that all the python dependencies are installed in the docker container.  Then we create the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myacienv = CondaDependencies.create(conda_packages=['scikit-learn','numpy','pandas']) #showing how to add libs as an example - not needed for this model.\n",
    "\n",
    "with open(\"mydeployenv.yml\",\"w\") as f:\n",
    "    f.write(myacienv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mydeployenv.yml\",\"r\") as f:\n",
    "  print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will take 10-15 minutes to finish\n",
    "\n",
    "service_name = \"myaci\"\n",
    "image_name = 'myimage'\n",
    "runtime = \"spark-py\" \n",
    "driver_file = \"score_sparkml.py\"\n",
    "my_conda_file = \"mydeployenv.yml\"\n",
    "\n",
    "# image creation\n",
    "from azureml.core.image import ContainerImage\n",
    "myimage_config = ContainerImage.image_configuration(execution_script = driver_file, \n",
    "                                    runtime = runtime, \n",
    "                                    conda_file = my_conda_file)\n",
    "\n",
    "# Create container Image\n",
    "myimage = ContainerImage.create(\n",
    "  workspace=ws, \n",
    "  name=image_name,\n",
    "  models = [mymodel],\n",
    "  image_config = myimage_config)\n",
    "\n",
    "myimage.wait_for_creation(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ContainerImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the actual webservice, using the Docker image that is stored in the Azure Container Registry. \n",
    "\n",
    "Before you continue, try to find your container image in the Azure portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy to ACI\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "\n",
    "myaci_config = AciWebservice.deploy_configuration(\n",
    "    cpu_cores = 2, \n",
    "    memory_gb = 2, \n",
    "    tags = {'name':'Databricks Azure ML ACI'}, \n",
    "    description = 'This is for ADB and AML example. Azure Databricks & Azure ML SDK demo with ACI.',\n",
    "    location='westus2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(azureml.core.webservice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webservice creation\n",
    "myservice = Webservice.deploy_from_image(\n",
    "  workspace=ws, \n",
    "  name=service_name,\n",
    "  image=myimage,\n",
    "  deployment_config = myaci_config)\n",
    "\n",
    "myservice.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we created above. Here is a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(myservice.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also print individual properties of your webservice, for example the URL used by the webservice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for using the Web HTTP API \n",
    "print(myservice.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the test_json data we created above. \n",
    "myservice.run(input_data = test_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment below line to not delete the web service\n",
    "myservice.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure to install **VS Code** on your compute *before* the next session tmrw morning.\n",
    "\n",
    "You can find binary installers for VS Code here:\n",
    "\n",
    "[https://code.visualstudio.com/download](https://code.visualstudio.com/download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  }
 ],
 "metadata": {
  "name": "04.DeploytoACI",
  "notebookId": 4057188818416571
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
