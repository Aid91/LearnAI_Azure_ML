{"cells":[{"cell_type":"markdown","source":["Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."],"metadata":{}},{"cell_type":"markdown","source":["# Model deployment"],"metadata":{}},{"cell_type":"markdown","source":["Please ensure you have run all previous notebooks in sequence before running this.\n\nPlease Register Azure Container Instance(ACI) using Azure Portal: https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-supported-services#portal in your subscription before using the SDK to deploy your ML model to ACI."],"metadata":{}},{"cell_type":"code","source":["from azureml.core import Workspace\nimport azureml.core\nimport os\n\n# Check core SDK version number\nprint(\"SDK version:\", azureml.core.VERSION)\n\nconfig_path = '/dbfs/tmp/'\n\n#'''\nws = Workspace.from_config(path=os.path.join(config_path, 'aml_config', 'config.json'))\nprint('Workspace name: ' + ws.name, \n      'Azure region: ' + ws.location, \n      'Resource group: ' + ws.resource_group, sep = '\\n')\n#'''"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["## NOTE: service deployment always gets the model from the current working dir.\nmodel_name = \"PdM_logistic_regression.mml\" # \nmodel_name_dbfs = os.path.join(\"/dbfs\", model_name)\n\nprint(\"copy model from dbfs to local\")\nmodel_local = \"file:\" + os.getcwd() + \"/\" + model_name\ndbutils.fs.cp(model_name, model_local, True)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# register the model\nfrom azureml.core.model import Model\nmymodel = Model.register(model_path = model_name, # this points to a local file\n                       model_name = model_name, # this is the name the model is registered as, am using same name for both path and name.                 \n                       description = \"ADB trained model by an amazing data scientist\",\n                       workspace = ws)\n\nprint(mymodel.name, mymodel.description, mymodel.version)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["## Converting your data to and from JSON\n\nThe most common way to interact with a webservice is using a [REST](https://en.wikipedia.org/wiki/Representational_state_transfer) API, sending and receiving [JSON](https://en.wikipedia.org/wiki/JSON) data.  \n\nWe therefore need to convert our dataframe to JSON to send it to the webservice, and the webservice has to then convert it back into a dataframe so that we can use our pyspark model to score the data.\n\nVery often this is straightforward, because json can interpret the schema of our data correctly. However, this is not always the case.  Our usecase is an example where we need to help spark, by explicitly providing the schema when converting the JSON data back to a dataframe.\n\nLet's start with an example to illustrate that.\n\n  **Note**: Explicitly providing the schema of data is generally good practice, because it can speed up reading data and avoids surprises.  This is not only try when working with spark, but also e.g. in *R*  or *scikit-learn*."],"metadata":{}},{"cell_type":"code","source":["df = spark.read.parquet(\"dbfs:/FileStore/tables/preprocessed\").cache()\ndisplay(df)\n\n# from pyspark.sql.types import DateType\nfrom pandas import datetime\nfrom pyspark.sql.functions import col, hour\n\n# we sample every nth row of the data using the `hour` function\ndf_train = df.filter((col('datetime') < datetime(2015, 10, 1)))\ndf_test = df.filter(col('datetime') > datetime(2015, 10, 15)).limit(5)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# test_data_path = \"TestData\"\n\n# test_data_path_dbfs = os.path.join(\"/dbfs\", test_data_path)\n\n# df_test = spark.read.parquet(test_data_path).limit(5)\n\ndisplay(df_test.limit(5))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["import json\n\ntest_json = json.dumps(df_test.toJSON().collect())\n\nprint(test_json)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["input_list = json.loads(test_json)\ninput_rdd = sc.parallelize(input_list)\ninput_df = spark.read.json(input_rdd)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Now, let's see whether the data look as expected after the rountrip though JSON."],"metadata":{}},{"cell_type":"code","source":["print(\"This is the schema of the original data frame:\")\ndf_test.printSchema()\n\nprint(\"This is the schema of our data frame after converting it to/from JSON:\")\ninput_df.printSchema()\n\ntry:\n  assert(df_test.schema == input_df.schema)\nexcept AssertionError:\n  print(\"Sadly, the schemas of the two data frames are not the same.\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## Hands-on Lab\n\nHelp spark by explicitly providing the schema when reading the JSON data.\n\nThis requires several parts:\n1. Identify the schema of the original data\n1. Create a schema definition that spark can use when reading the JSON data\n1. Tell spark to use that schema definition when reading the JSON data"],"metadata":{}},{"cell_type":"code","source":["# Let's identify the schma\ndf_test.schema"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["OK. It looks like:\n- `norm_features` are encoded as a `VectorUDT`\n- `error` is encoded as `IntegerType`\n\nThe schema definition further depends on the classes `StructType` and `StructField`.\n\nTry to find where those are defined using the pyspark API, and add the import statements at the top of the next cell. Hint, you need two lines of code.\n\nUse the search function of the pyspark API [documentation](https://spark.apache.org/docs/latest/api/python/index.html) to find the location of most of the definitions of these classes. Unfortunately, `VectorUDT` is a little bit harder to find, and will require some finesse on your side."],"metadata":{}},{"cell_type":"code","source":["#from pyspark.<...> import <...>\n#from pyspark.<...> import <...>\n\nmyschema = StructType([\n                      StructField(\"norm_features\",VectorUDT()),\n                      StructField(\"error\",IntegerType())\n                      ])"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["from pyspark.sql.types import StructField, StructType, IntegerType\nfrom pyspark.ml.linalg import VectorUDT\n\nmyschema = StructType([\n                      StructField(\"norm_features\",VectorUDT()),\n                      StructField(\"error\",IntegerType())\n                      ])"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Now that you were able to define the schema, tell spark to use it when reading the JSON data.\n\nInstead of simply writing `spark.read.json(input_rdd)`, tell spark to use your schema while reading the data.\n\nUse this [documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=spark%20read%20schema#pyspark.sql.DataFrameReader.schema) for some hint on how to do this."],"metadata":{}},{"cell_type":"code","source":["input_list = json.loads(test_json)\ninput_rdd = sc.parallelize(input_list)\n# todo: modify the next line \ninput_df = spark.read.json(input_rdd)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["input_list = json.loads(test_json)\ninput_rdd = sc.parallelize(input_list)\n# todo: modify the next line \ninput_df = spark.read.schema(myschema).json(input_rdd)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Now, let's see whether you were successful."],"metadata":{}},{"cell_type":"code","source":["print(\"This is the schema of the original data frame:\")\ndf_test.printSchema()\n\nprint(\"This is the schema of our data frame after converting it to/from JSON:\")\ninput_df.printSchema()\n\ntry:\n  assert(df_test.schema == input_df.schema)\n  print(\"You did it!\")\nexcept AssertionError:\n  print(\"Sadly, the schemas of the two data frames are not the same.\")"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["## End of lab\n\n## Create a score file\n\nThe next step of creating a web service is to define a score script that defines what the webservice does.\n\nA typical score script has two methods defined:\n- `init` is executed once, when the webservice is started\n- `run` is executed everytime a user is interacting with the webservice to score data\n\nLook at this score script below, can you see where we made the changes that are related to explicitly providing the schema when reading JSON data?\n\nThere are several places:\n1. Importing the modules for defining the schema\n1. Defining a global variable for holding the schema\n1. Defining the schema\n1. Using the schema when reading the data"],"metadata":{}},{"cell_type":"code","source":["score_sparkml = \"\"\"\n\nimport json\n\ndef init():\n    # One-time initialization of PySpark and predictive model\n    import pyspark\n    from azureml.core.model import Model\n    from pyspark.ml import PipelineModel\n    from pyspark.sql.types import StructField, StructType, IntegerType\n    from pyspark.ml.linalg import VectorUDT\n\n    global trainedModel\n    global spark\n    global schema\n    \n    spark = pyspark.sql.SparkSession.builder.appName(\"ADB and AML notebook by an amazing data scientist\").getOrCreate()\n    model_name = \"{model_name}\" #interpolated\n    model_path = Model.get_model_path(model_name)\n    trainedModel = PipelineModel.load(model_path)\n    \n    schema = StructType([StructField(\"norm_features\",VectorUDT()), StructField(\"error\",IntegerType())])\n    \ndef run(input_json):\n    if isinstance(trainedModel, Exception):\n        return json.dumps({{\"trainedModel\":str(trainedModel)}})\n      \n    try:\n        sc = spark.sparkContext\n        input_list = json.loads(input_json)\n        input_rdd = sc.parallelize(input_list)\n        input_df = spark.read.schema(schema).json(input_rdd)\n        \n        # Compute prediction\n        prediction = trainedModel.transform(input_df)\n        #result = prediction.first().prediction\n        predictions = prediction.collect()\n\n        #Get each scored result\n        preds = [str(x['prediction']) for x in predictions]\n        result = \",\".join(preds)\n        # you can return any data type as long as it is JSON-serializable\n        return json.dumps({{\"result\":result}})        \n    except Exception as e:\n        result = str(e)\n        return json.dumps({{\"error\":result}})\n    \n\"\"\".format(model_name=model_name)\n\nexec(score_sparkml)\n\nwith open(\"score_sparkml.py\", \"w\") as file:\n    file.write(score_sparkml)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Creating a webservice requires creating a docker container in which to run our score script. \n\nThis can all be done with the python AML sdk. \n\nFirst we create a conda environment, which makes sure that all the python dependencies are installed in the docker container.  Then we create the container."],"metadata":{}},{"cell_type":"code","source":["from azureml.core.conda_dependencies import CondaDependencies \n\nmyacienv = CondaDependencies.create(conda_packages=['scikit-learn','numpy','pandas']) #showing how to add libs as an example - not needed for this model.\n\nwith open(\"mydeployenv.yml\",\"w\") as f:\n    f.write(myacienv.serialize_to_string())"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["with open(\"mydeployenv.yml\",\"r\") as f:\n  print(f.read())"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# this will take 10-15 minutes to finish\n\nservice_name = \"myaci\"\nimage_name = 'myimage'\nruntime = \"spark-py\" \ndriver_file = \"score_sparkml.py\"\nmy_conda_file = \"mydeployenv.yml\"\n\n# image creation\nfrom azureml.core.image import ContainerImage\nmyimage_config = ContainerImage.image_configuration(execution_script = driver_file, \n                                    runtime = runtime, \n                                    conda_file = my_conda_file)\n\n# Create container Image\nmyimage = ContainerImage.create(\n  workspace=ws, \n  name=image_name,\n  models = [mymodel],\n  image_config = myimage_config)\n\nmyimage.wait_for_creation(show_output=True)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["help(ContainerImage)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["Now we create the actual webservice, using the Docker image that is stored in the Azure Container Registry. \n\nBefore you continue, try to find your container image in the Azure portal."],"metadata":{}},{"cell_type":"code","source":["# deploy to ACI\nfrom azureml.core.webservice import AciWebservice, Webservice\n\nmyaci_config = AciWebservice.deploy_configuration(\n    cpu_cores = 2, \n    memory_gb = 2, \n    tags = {'name':'Databricks Azure ML ACI'}, \n    description = 'This is for ADB and AML example. Azure Databricks & Azure ML SDK demo with ACI.',\n    location='westus2')"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["help(azureml.core.webservice)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# Webservice creation\nmyservice = Webservice.deploy_from_image(\n  workspace=ws, \n  name=service_name,\n  image=myimage,\n  deployment_config = myaci_config)\n\nmyservice.wait_for_deployment(show_output=True)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["Let's see what we created above. Here is a summary."],"metadata":{}},{"cell_type":"code","source":["print(myservice.serialize())"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["You can also print individual properties of your webservice, for example the URL used by the webservice."],"metadata":{}},{"cell_type":"code","source":["#for using the Web HTTP API \nprint(myservice.scoring_uri)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["## Test Webservice"],"metadata":{}},{"cell_type":"code","source":["# We can use the test_json data we created above. \nmyservice.run(input_data = test_json)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# comment below line to not delete the web service\nmyservice.delete()"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["Please make sure to install **VS Code** on your compute *before* the next session tmrw morning.\n\nYou can find binary installers for VS Code here:\n\n[https://code.visualstudio.com/download](https://code.visualstudio.com/download)"],"metadata":{}},{"cell_type":"markdown","source":["Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."],"metadata":{}}],"metadata":{"name":"04.DeploytoACI","notebookId":4057188818416571},"nbformat":4,"nbformat_minor":0}