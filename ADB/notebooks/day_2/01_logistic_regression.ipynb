{"cells":[{"cell_type":"markdown","source":["Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n#Logistic Regression Lab"],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"../includes/setup_env\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Reading the data"],"metadata":{}},{"cell_type":"markdown","source":["We begin by reading the data that we finished pre-processing in a prior Notebook.\n\n*Note:* If you you do get an error messages about a non-existent file, please uncomment the first row of the following cell. This will run yesterday's notebook for preparing our dataset."],"metadata":{}},{"cell_type":"code","source":["# %run \"../day_1/03_data_prep_lab\"\n\ndf = spark.read.parquet(\"dbfs:/FileStore/tables/preprocessed\").cache()\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Let's begin by dividing the data into training and test sets. With time-series data, we usually divide the data based on a time cut-off and to avoid **leakage** we also put a gap (2 weeks in this case) between the training and test data. Another option we have is to sample every n-th row of the data. The data is collected hourly, and if we do not wish to use such a high frequency for modeling, we can sample every n-th row of the data."],"metadata":{}},{"cell_type":"code","source":["# from pyspark.sql.types import DateType\nfrom pandas import datetime\nfrom pyspark.sql.functions import col, hour\n\n# we sample every nth row of the data using the `hour` function\ndf_train = df.filter((col('datetime') < datetime(2015, 10, 1))) # & (hour(col('datetime')) % 3 == 0))\ndf_test = df.filter(col('datetime') > datetime(2015, 10, 15))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Let's look at some summary statistics for the labels in the data."],"metadata":{}},{"cell_type":"code","source":["display(df_train.describe())"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["We now build a classifier for `y_0` (failure in the first component) (and drop the other labels)."],"metadata":{}},{"cell_type":"code","source":["df_train = df_train.drop(\"y_1\",\"y_2\",\"y_3\",\"datetime\", \"machineID\")\ndf_train = df_train.withColumnRenamed(\"y_0\", \"error\")\ndf_train.cache()\n\ndf_test = df_test.drop(\"y_1\",\"y_2\",\"y_3\",\"datetime\", \"machineID\")\ndf_test = df_test.withColumnRenamed(\"y_0\", \"error\")\ndf_test.cache()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Let's make sure we don't have any null values in our DataFrame."],"metadata":{}},{"cell_type":"code","source":["recordCount = df_train.count()\nnoNullsRecordCount = df_train.na.drop().count()\n\nprint(\"We have {} records that contain null values.\".format(recordCount - noNullsRecordCount))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["display(df_train.groupBy(\"error\").count())"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## Train a Logistic Regression Model\n\nBefore we can apply the logistic regression model, we will need to do some data preparation, such as one hot encoding our categorical variables using `StringIndexer` and `OneHotEncoderEstimator`.\n\nLet's start by taking a look at all of our columns, and determine which ones are categorical."],"metadata":{}},{"cell_type":"code","source":["df_train.printSchema()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["## Setting up the model\n\nWe set the `label` column of the LogisticRegression model to `error`, and the `features` column to `norm_features`."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\nlr = (LogisticRegression()\n     .setLabelCol(\"error\")\n     .setFeaturesCol(\"norm_features\"))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Hands-on lab\nCreate a pipeline that contains a single stage for the model we created above. Then fit the pipeline to the training data and then use the fitted model to `transform` the test data."],"metadata":{}},{"cell_type":"code","source":["# maximize this cell (click the + button on the right) to see the solution:\n  \nfrom pyspark.ml import Pipeline\n\npipeline = Pipeline(stages = [lr])\nassert len(pipeline.getStages()) == 1 # make sure it's one stage only\nprint(pipeline.getStages())\n\nlr_model = pipeline.fit(df_train)\n\ndf_pred = lr_model.transform(df_test) # apply the model to our held-out test set\ndisplay(df_pred)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### End of lab"],"metadata":{}},{"cell_type":"code","source":["df_pred.printSchema()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Evaluate the Model"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nevaluator = BinaryClassificationEvaluator()\nprint(evaluator.explainParams())"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["evaluator.setLabelCol(\"error\")\nevaluator.setRawPredictionCol('rawPrediction')\n\nmetricName = evaluator.getMetricName()\nmetricVal = evaluator.evaluate(df_pred)\n\nprint(\"{}: {}\".format(metricName, metricVal))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["We could wrap this into a function to make it easier to get the output of multiple metrics."],"metadata":{}},{"cell_type":"code","source":["def printEval(df, labelCol = \"error\", rawPredictionCol = \"rawPrediction\"):\n  evaluator = BinaryClassificationEvaluator()\n  evaluator.setLabelCol(labelCol)\n  evaluator.setRawPredictionCol(rawPredictionCol)\n\n  auroc = evaluator.setMetricName(\"areaUnderROC\").evaluate(df)\n  print(\"AUROC: {}\".format(auroc))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["printEval(df_pred)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["##Conclusion\nHmmmm... our results are not great yet. We'll look into how to improve our results later."],"metadata":{}},{"cell_type":"markdown","source":["Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."],"metadata":{}}],"metadata":{"name":"01_logistic_regression","notebookId":4057188818416322},"nbformat":4,"nbformat_minor":0}