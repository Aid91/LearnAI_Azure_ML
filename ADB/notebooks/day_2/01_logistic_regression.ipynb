{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-sandbox\n",
    "#Logistic Regression Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n",
    "\n",
    "Run the following cell to configure our \"classroom.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../includes/setup_env\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by reading the data that we finished pre-processing in a prior Notebook.\n",
    "\n",
    "*Note:* If you you do get an error messages about a non-existent file, please uncomment the first row of the following cell. This will run yesterday's notebook for preparing our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run \"../day_1/03_data_prep_lab\"\n",
    "\n",
    "df = spark.read.parquet(\"dbfs:/FileStore/tables/preprocessed\").cache()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by dividing the data into training and test sets. With time-series data, we usually divide the data based on a time cut-off and to avoid **leakage** we also put a gap (2 weeks in this case) between the training and test data. Another option we have is to sample every n-th row of the data. The data is collected hourly, and if we do not wish to use such a high frequency for modeling, we can sample every n-th row of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import DateType\n",
    "from pandas import datetime\n",
    "from pyspark.sql.functions import col, hour\n",
    "\n",
    "# we sample every nth row of the data using the `hour` function\n",
    "df_train = df.filter((col('datetime') < datetime(2015, 10, 1))) # & (hour(col('datetime')) % 3 == 0))\n",
    "df_test = df.filter(col('datetime') > datetime(2015, 10, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some summary statistics for the labels in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build a classifier for `y_0` (failure in the first component) (and drop the other labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(\"y_1\",\"y_2\",\"y_3\",\"datetime\", \"machineID\")\n",
    "df_train = df_train.withColumnRenamed(\"y_0\", \"error\")\n",
    "df_train.cache()\n",
    "\n",
    "df_test = df_test.drop(\"y_1\",\"y_2\",\"y_3\",\"datetime\", \"machineID\")\n",
    "df_test = df_test.withColumnRenamed(\"y_0\", \"error\")\n",
    "df_test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we don't have any null values in our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordCount = df_train.count()\n",
    "noNullsRecordCount = df_train.na.drop().count()\n",
    "\n",
    "print(\"We have {} records that contain null values.\".format(recordCount - noNullsRecordCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train.groupBy(\"error\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Logistic Regression Model\n",
    "\n",
    "Before we can apply the logistic regression model, we will need to do some data preparation, such as one hot encoding our categorical variables using `StringIndexer` and `OneHotEncoderEstimator`.\n",
    "\n",
    "Let's start by taking a look at all of our columns, and determine which ones are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "\n",
    "We set the `label` column of the LogisticRegression model to `error`, and the `features` column to `norm_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = (LogisticRegression()\n",
    "     .setLabelCol(\"error\")\n",
    "     .setFeaturesCol(\"norm_features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on lab\n",
    "Create a pipeline that contains a single stage for the model we created above. Then fit the pipeline to the training data and then use the fitted model to `transform` the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximize this cell (click the + button on the right) to see the solution:\n",
    "  \n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages = [lr])\n",
    "assert len(pipeline.getStages()) == 1 # make sure it's one stage only\n",
    "print(pipeline.getStages())\n",
    "\n",
    "lr_model = pipeline.fit(df_train)\n",
    "\n",
    "df_pred = lr_model.transform(df_test) # apply the model to our held-out test set\n",
    "display(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(evaluator.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.setLabelCol(\"error\")\n",
    "evaluator.setRawPredictionCol('rawPrediction')\n",
    "\n",
    "metricName = evaluator.getMetricName()\n",
    "metricVal = evaluator.evaluate(df_pred)\n",
    "\n",
    "print(\"{}: {}\".format(metricName, metricVal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could wrap this into a function to make it easier to get the output of multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printEval(df, labelCol = \"error\", rawPredictionCol = \"rawPrediction\"):\n",
    "  evaluator = BinaryClassificationEvaluator()\n",
    "  evaluator.setLabelCol(labelCol)\n",
    "  evaluator.setRawPredictionCol(rawPredictionCol)\n",
    "\n",
    "  auroc = evaluator.setMetricName(\"areaUnderROC\").evaluate(df)\n",
    "  print(\"AUROC: {}\".format(auroc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printEval(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Conclusion\n",
    "Hmmmm... our results are not great yet. We'll look into how to improve our results later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  }
 ],
 "metadata": {
  "name": "01_logistic_regression",
  "notebookId": 4057188818416322
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
