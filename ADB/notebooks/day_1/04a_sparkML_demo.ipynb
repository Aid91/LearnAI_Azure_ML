{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Model Development with Spark\n",
    "\n",
    "This will be the first of three parts of a bootcamp on Model Development with [MLlib](https://spark.apache.org/docs/latest/ml-guide.html), Sparkâ€™s machine learning (ML) library.  You will gain hands-on experience with essential steps of a model development using MLlib, which has has the goal to make machine learning scalable and easy. \n",
    "\n",
    "At a high level, MLlib provides tools such as:\n",
    "- ML Algorithms: common learning algorithms such as classification, regression, clustering\n",
    "- Featurization: feature extraction, transformation, dimensionality reduction, and selection\n",
    "- Pipelines: tools for constructing, evaluating, and tuning ML Pipelines\n",
    "- Persistence: saving and load algorithms, models, and Pipelines\n",
    "- Utilities: linear algebra, statistics, data handling, etc.\n",
    "\n",
    "In this lab, we will cover:\n",
    "- Splitting of data for training and testing\n",
    "- Applying Transformers to data frames\n",
    "- Fitting Estimators to our data\n",
    "- Creating and executing a ML Pipeline\n",
    "- Model Evaluation.\n",
    "\n",
    "We will learn how to use these features and tools by solving a common task in Natural Language Processing (NLP): Sentiment Analysis.  Our dataset contains roughly 50,000 movie reviews from the internet movie database (IMDB).  Each entry is a movie review written in the English language, as well as the author's rating of the movie on a scale from 1 to 10.  Based on the text of the review, we want to predict if the rating is \"positive\" or \"negative\".\n",
    "\n",
    "Other applications of Sentiment Analysis:\n",
    "- Detecting negative affect in customers who are calling an automated customer hotline\n",
    "- Agreggating reviews of retail products into an overall rating for each product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Notebook Environment\n",
    "\n",
    "As usual we start by mounting our data and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../includes/mnt_blob\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../includes/setup_env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsDF = spark.read.parquet(\"/mnt/data/imdb/imdb_ratings_50k.parquet\")\n",
    "reviewsDF.createOrReplaceTempView(\"reviews\")\n",
    "display(reviewsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the distribution of scores look like?\n",
    "\n",
    "HINT: Use `count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- Put your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- Maximize this cell to see the solution\n",
    "\n",
    "select rating, count(*) from reviews group by rating order by rating;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors of this dataset have removed the \"neutral\" ratings, which they defined as a rating of 5 or 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "We'll split our data into training and test samples. We will use 80% for training, and the remaining 20% for testing. We set a seed to reproduce the same results (i.e. if you re-run this notebook, you'll get the same results both times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainDF, testDF) = reviewsDF.randomSplit([0.8, 0.2], seed=42)\n",
    "trainDF.cache()\n",
    "testDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine our baseline accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveRatings = trainDF.filter(\"rating >= 5\").count()\n",
    "totalRatings = trainDF.count()\n",
    "\n",
    "print(\"Baseline accuracy: {0:.2f}%\".format(positiveRatings/totalRatings*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "A transformer takes in a DataFrame, and returns a new DataFrame with one or more columns appended to it. They implement a `.transform()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started by using [RegexTokenizer](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.RegexTokenizer) to convert our review string into a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "tokenizer = (RegexTokenizer()\n",
    "            .setInputCol(\"review\")\n",
    "            .setOutputCol(\"tokens\")\n",
    "            .setPattern(\"\\\\W+\"))\n",
    "\n",
    "tokenizedDF = tokenizer.transform(reviewsDF)\n",
    "\n",
    "display(tokenizedDF.limit(5)) # Look at a few tokenized reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of words that do not contain much information about the sentiment of the review (e.g. `the`, `a`, etc.). Let's remove these uninformative words using [StopWordsRemover](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = (StopWordsRemover()\n",
    "          .setInputCol(\"tokens\")\n",
    "          .setOutputCol(\"stopWordFree\"))\n",
    "\n",
    "removedStopWordsDF = remover.transform(tokenizedDF)\n",
    "display(removedStopWordsDF.limit(5)) # Look at a few tokenized reviews without stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hands-on lab\n",
    "\n",
    "Where do the stop words actually come from? Spark includes a small English list as a default, which we're implicitly using here.\n",
    "\n",
    "Look into the Spark documentation, to find out how you can get a list of the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes into this cell\n",
    "stopwords = []\n",
    "stopwords # this prints the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gives you a list of the stopwords                                                                                                                                                                  \n",
    "\n",
    "stopWords = remover.getStopWords()\n",
    "stopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to remove additional stop words. For example, let's remove each occurrence of the `br` from the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution\n",
    "removedStopWordsDF = remover.transform(tokenizedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these two lines will remove \"br\" and all other defined stop words\n",
    "remover.setStopWords([\"br\", \"nbsp\"] + stopWords)\n",
    "removedStopWordsDF = remover.transform(tokenizedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(removedStopWordsDF.limit(5)) # Look at a few tokenized reviews without stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators\n",
    "\n",
    "Estimators take in a DataFrame, and return a model (a Transformer). They implement a `.fit()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply a [CountVectorizer](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.CountVectorizer) model to convert our tokens into a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "counts = (CountVectorizer()\n",
    "          .setInputCol(\"stopWordFree\")\n",
    "          .setOutputCol(\"features\")\n",
    "          .setVocabSize(1000))\n",
    "\n",
    "countModel = counts.fit(removedStopWordsDF) # It's a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now let's adjust the label (target) values__\n",
    "\n",
    "We want to group the reviews into \"positive\" or \"negative\" sentiment. So all of the star \"levels\" need to be collapsed into one of two groups. To accomplish this, we will use [Binarizer](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Binarizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "binarizer = (Binarizer()\n",
    "            .setInputCol(\"rating\")\n",
    "            .setOutputCol(\"label\")\n",
    "            .setThreshold(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use a Decision Tree model to fit to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "Let's put all of these stages into a [Pipeline](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Pipeline). This way, you don't have to remember all of the different steps you applied to the training set, and then apply the same steps to the test dataset. The pipeline takes care of that for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline().setStages([tokenizer, remover, counts, binarizer, dtc])\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the stages from our Pipeline, such as the Decision Tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTree = pipelineModel.stages[-1]\n",
    "print(decisionTree.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on lab (optional!)\n",
    "\n",
    "Please skip this hands-on lab the first time you go through this notebook.\n",
    "\n",
    "After you fitted your first pipeline that included a step with the `CountVectorizer`, you could try to improve the performance of you model, by substituting the `CountVectorizer` with `Word2Vec`, which creates a more sophisticated semantic representation than simply counting the frequency of words in each review. Make sure to also update the pipeline definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# word2Vec = TODO: define you word2vec estimator here\n",
    "\n",
    "# TODO: update the below pipeline definition, to make sure it uses word2Vec\n",
    "pipeline = Pipeline().setStages([tokenizer, remover, counts, binarizer, dtc])\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done defining your word2vec estimator and updating your pipeline. Run *all* the below cells again, and see whether the performance of your model has changed.\n",
    "\n",
    "### End of (optional) hands-on lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the pipeline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = userhome + \"/tmp/DT_Pipeline\"\n",
    "pipelineModel.write().overwrite().save(fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the `PipelineModel` back in.\n",
    "\n",
    "**Note**: You need to know what type of model you're loading in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "# Load saved model\n",
    "savedPipelineModel = PipelineModel.load(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF = savedPipelineModel.transform(testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "We are going to use [MultiClassClassificationEvaluator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator)  to evaluate our predictions (we are using MultiClass because the BinaryClassificationEvaluator does not support accuracy as a metric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Accuracy: %(result)s\" % {\"result\": evaluator.evaluate(resultDF)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "Let's see if we had more False Positive or False Negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(resultDF.groupBy(\"label\", \"prediction\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we will see how to apply this pipeline to streaming data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-sandbox\n",
    "&copy; 2018 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "name": "04a_sparkML_demo",
  "notebookId": 4057188818416202
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
