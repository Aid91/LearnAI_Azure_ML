{"cells":[{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Decision Trees\n### Analyzing a bike sharing dataset\n\nThis notebook demonstrates creating an ML Pipeline to preprocess a dataset, train a Machine Learning model, save the model, and make predictions.\n\n**Data**: The dataset contains bike rental info from 2011 and 2012 in the Capital bikeshare system, plus additional relevant information such as weather.  This dataset is from Fanaee-T and Gama (2013) and is hosted by the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset).\n\n**Goal**: We want to learn to predict bike rental counts (per hour) from information such as day of the week, weather, season, etc.  Having good predictions of customer demand allows a business or service to prepare and increase supply as needed.  \n\nIn the next lab, we will also demonstrate hyperparameter tuning using cross-validation, as well as tree ensembles to fine-tune and improve our ML model.\n\n[Decision Tree Regressor (Scala)](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.regression.DecisionTreeRegressor)\n\n[Decision Tree Regressor (Python)](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor)"],"metadata":{}},{"cell_type":"code","source":["%run \"../includes/mnt_blob\""],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["## Load and understand the data\n\nWe begin by loading our data, which is stored in [Comma-Separated Value (CSV) format](https://en.wikipedia.org/wiki/Comma-separated_values). \n\nUse the `spark.read.csv` method to read the data and set a few options:\n- `header`: set to true to indicate that the first line of the CSV data file is a header\n- `inferSchema`: set to true to infer the datatypes\n- The file is located at `/mnt/data/bikeSharing/data-001/hour.csv`.\n\n[DataFrame Reader (Scala)](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.DataFrameReader)\n\n[DataFrame Reader (Python)](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader)"],"metadata":{}},{"cell_type":"code","source":["# TODO\n# df = spark.read.<FILL_IN>"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["Let's cache the DataFrame so subsequent uses will be able to read from memory, instead of re-reading the data from disk."],"metadata":{}},{"cell_type":"code","source":["# df.cache()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["__Question__: Is the DataFrame in the Storage tab of the Spark UI?"],"metadata":{}},{"cell_type":"markdown","source":["#### Data description\n\nFrom the [UCI ML Repository description](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset), we have the following schema.\n\n**Feature columns**:\n* dteday: date\n* season: season (1:spring, 2:summer, 3:fall, 4:winter)\n* yr: year (0:2011, 1:2012)\n* mnth: month (1 to 12)\n* hr: hour (0 to 23)\n* holiday: whether day is holiday or not\n* weekday: day of the week\n* workingday: if day is neither weekend nor holiday is 1, otherwise is 0.\n* weathersit: \n  * 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n  * 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n  * 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n  * 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n* temp: Normalized temperature in Celsius. The values are derived via `(t-t_min)/(t_max-t_min)`, `t_min=-8`, `t_max=+39` (only in hourly scale)\n* atemp: Normalized feeling temperature in Celsius. The values are derived via `(t-t_min)/(t_max-t_min)`, `t_min=-16`, `t_max=+50` (only in hourly scale)\n* hum: Normalized humidity. The values are divided to 100 (max)\n* windspeed: Normalized wind speed. The values are divided to 67 (max)\n\n**Label columns**:\n* casual: count of casual users\n* registered: count of registered users\n* cnt: count of total rental bikes including both casual and registered\n\n**Extraneous columns**:\n* instant: record index\n\nFor example, the first row is a record of hour 0 on January 1, 2011---and apparently 16 people rented bikes around midnight!"],"metadata":{}},{"cell_type":"markdown","source":["Let's look at a subset of our data. We'll use the [sample()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sample) method to sample 10% of the DataFrame without replacement, and call `display()` on the resulting DataFrame."],"metadata":{}},{"cell_type":"code","source":["# TODO\ndisplay(<FILL_IN>)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## Preprocess data\n\nSo what do we need to do to get our data ready for Machine Learning?\n\n*Recall our goal*: We want to learn to predict the count of bike rentals (the `cnt` column).  We refer to the count as our target \"label\".\n\n*Features*: What can we use as features to predict the `cnt` label?  All the columns except `cnt`, and a few exceptions:\n* The `cnt` column we want to predict equals the sum of the `casual` + `registered` columns.  We will remove the `casual` and `registered` columns from the data to make sure we do not use them to predict `cnt`.  (*Warning: This is a danger in careless Machine Learning.  Make sure you do not \"cheat\" by using information you will not have when making predictions*)\n* date column `dteday`: We could keep it, but it is well-represented by the other date-related columns `season`, `yr`, `mnth`, and `weekday`.  We will discard it.\n* `holiday` and `weekday`: These features are highly correlated with the `workingday` column.\n* row index column `instant`: This is a useless column to us."],"metadata":{}},{"cell_type":"markdown","source":["Let's drop the columns `instant`, `dteday`, `casual`, `holiday`, `weekday`, and `registered` from our DataFrame."],"metadata":{}},{"cell_type":"code","source":["# TODO\ndf = df.<FILL_IN>\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Now that we have the columns we care about, let's print the schema of our dataset to see the type of each column using `printSchema()`."],"metadata":{}},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Train/Test Split\n\nOur final data preparation step will be to split our dataset into separate training and test sets.\n\nUse `randomSplit` to split the data such that 70% of the data is reserved for training, and the remaining 30% for testing. Use the set `seed` for reproducability (i.e. if you re-run this notebook or compare with your neighbor, you will get the same results).\n\nPython: [randomSplit()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit)\n\nScala: [randomSplit()](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.Dataset)"],"metadata":{}},{"cell_type":"code","source":["# TODO\nseed = 42\ntrainDF, testDF = df.<FILL_IN>\n\nprint(\"We have %d training examples and %d test examples.\" % (trainDF.count(), testDF.count()))\nassert (trainDF.count() == 12197)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["#### Visualize our data\n\nNow that we have preprocessed our features and prepared a training dataset, we can quickly visualize our data to get a sense of whether the features are meaningful.\n\nCalling `display()` on a DataFrame in Databricks and clicking the plot icon below the table will let you draw and pivot various plots.  See the [Visualizations section of the Databricks Guide](https://docs.databricks.com/user-guide/visualizations/index.html) for more ideas.\n\nWe want to compare bike rental counts versus hour of the day.  As one might expect, rentals are low during the night, and they peak in the morning (8am) and in the early evening (6pm).  This indicates the `hr` feature is useful and can help us predict our label `cnt`.  \n\nSelect the `hr` and `cnt` columns from `trainDF`, and visualize it as a bar chart (you might need to adjust the plot options)."],"metadata":{}},{"cell_type":"code","source":["# TODO\ndisplay(<FILL_IN>)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["## Train a Machine Learning Pipeline\n\nLet's learn a ML model to predict the `cnt` of bike rentals given a single `features` column of feature vectors. \n\nWe will put together a simple Pipeline with the following stages:\n* `VectorAssembler`: Assemble the feature columns into a feature vector.\n* `VectorIndexer`: Identify columns which should be treated as categorical.  This is done heuristically, identifying any column with a small number of distinct values as being categorical.  For us, this will be the `yr` (2 values), `season` (4 values), `holiday` (2 values), `workingday` (2 values), and `weathersit` (4 values).\n* `DecisionTreeRegressor`: This will build a decision tree to learn how to predict rental counts from the feature vectors."],"metadata":{}},{"cell_type":"markdown","source":["First, we define the feature processing stages of the Pipeline:\n* Assemble feature columns into a feature vector.\n* Identify categorical features, and index them.\n\n![Image of feature processing](http://training.databricks.com/databricks_guide/2-features.png)"],"metadata":{}},{"cell_type":"markdown","source":["Steps:\n- To create our feature vector, we start by selecting all of the feature columns and calling it `featuresCols`.\n- Use [VectorAssembler](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler), and set `inputCols` to `featureCols`, and the `outputCols` as `rawFeatures`. This concatenates all feature columns into a single feature vector into the new column \"rawFeatures\".\n- Use [VectorIndexer](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorIndexer) to identify categorical features in our `rawFeatures` and index them. If any column has `maxCategories` or fewer distinct values, then it is treated as a categorical variable."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler, VectorIndexer\n\nfeaturesCols = df.columns[:-1] # Removes \"cnt\"\n\nvectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"rawFeatures\")\n\nvectorIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=4)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Decision Tree Regressor\nSecond, we define the model training stage of the Pipeline. [Decision Tree Regressor](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor) takes feature vectors and labels as input and learns to predict labels of new examples.\n\nLet's take a look at some of the default parameters."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import DecisionTreeRegressor\n\ndt = DecisionTreeRegressor()\nprint(dt.explainParams())"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["DecisionTreeRegressor expects a `labelCol` called `label`, but in our DataFrame we don't have a label column. Let's tell the DecisionTreeRegressor that the label column is called `cnt`.\n\nUse `dt.setLabelCol(\"\")`"],"metadata":{}},{"cell_type":"code","source":["# TODO"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Pipeline\nNow let's wrap all of these stages into a Pipeline."],"metadata":{}},{"cell_type":"code","source":["# TODO\nfrom pyspark.ml import Pipeline\n\npipeline = Pipeline().setStages([<FILL_IN>])"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Train\n\nTrain the pipeline model to run all the steps in the pipeline."],"metadata":{}},{"cell_type":"code","source":["pipelineModel = pipeline.fit(trainDF)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["Let's visualize the decision tree"],"metadata":{}},{"cell_type":"code","source":["print(pipelineModel.stages[2].toDebugString)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Save Model\n\nLet's go ahead and save this model."],"metadata":{}},{"cell_type":"code","source":["fileName = userhome + \"/tmp/MyPipeline\"\npipelineModel.write().overwrite().save(fileName)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["Let's read this model back in."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import PipelineModel\n\nsavedModel = PipelineModel.load(fileName)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Predictions\n\nNext, apply the saved model trained to the test set."],"metadata":{}},{"cell_type":"code","source":["# TODO\npredictionsDF = savedModel.<FILL_IN>\n\ndisplay(predictionsDF.select(\"cnt\", \"prediction\"))"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Evaluate\n\nNext, we'll use [RegressionEvaluator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator) to assess the results. The default regression metric is RMSE."],"metadata":{}},{"cell_type":"code","source":["# TODO\nfrom <FILL_IN>\n\nevaluator = <FILL_IN>\n\nrmse = <FILL_IN>\nprint(\"Test RMSE = %f\" % rmse)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Next Steps\n\nWow! Our RMSE is really high. In the next lab, we will cover ways to decrease the RMSE of our model, including: cross validation, hyperparameter tuning, and ensembles of trees."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2018 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"05_sparkML_lab","notebookId":4057188818416260},"nbformat":4,"nbformat_minor":0}