{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkML on Streaming Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take in the model we saved earlier, and apply it to some streaming data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../includes/mnt_blob\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../includes/setup_env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "fileName = userhome + \"/tmp/DT_Pipeline\"\n",
    "pipelineModel = PipelineModel.load(fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simulate streaming data.\n",
    "\n",
    "**Note**: You must specify a schema when creating a streaming source DataFrame. Why!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"rating\",DoubleType()), \n",
    "  StructField(\"review\",StringType())])\n",
    "\n",
    "streamingData = (spark\n",
    "                 .readStream\n",
    "                 .schema(schema)\n",
    "                 .option(\"maxFilesPerTrigger\", 1)\n",
    "                 .parquet(\"/mnt/data/imdb/imdb_ratings_50k.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this stream taking so long? What configuration should we set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = (pipelineModel\n",
    "          .transform(streamingData)\n",
    "          .groupBy(\"label\", \"prediction\")\n",
    "          .count()\n",
    "          .sort(\"label\", \"prediction\"))\n",
    "\n",
    "display(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = (pipelineModel\n",
    "          .transform(streamingData)\n",
    "          .groupBy(\"label\", \"prediction\")\n",
    "          .count()\n",
    "          .sort(\"label\", \"prediction\"))\n",
    "\n",
    "display(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our results to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "streamingView = re.sub('\\W', '', username)\n",
    "checkpointFile = userhome + \"/tmp/checkPoint\"\n",
    "dbutils.fs.rm(checkpointFile, True) # Clear out the checkpointing directory\n",
    "\n",
    "(stream\n",
    " .writeStream\n",
    " .format(\"memory\")\n",
    " .option(\"checkpointLocation\", checkpointFile)\n",
    " .outputMode(\"complete\")\n",
    " .queryName(streamingView)\n",
    " .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sql(\"select * from \" + streamingView))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about streaming [here](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-sandbox\n",
    "&copy; 2018 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "name": "04b_sparkML_streaming",
  "notebookId": 4057188818416178
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
