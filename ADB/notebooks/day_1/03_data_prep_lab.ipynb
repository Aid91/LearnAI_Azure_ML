{"cells":[{"cell_type":"markdown","source":["Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."],"metadata":{}},{"cell_type":"markdown","source":["# Data preperation and feature engineering for PdM\n\nAll of our labs use predictive maintenance as their use-case. In this lab, we read and pre-process the data. The relevant data sources for predictive maintenance include, but are not limited to:\n\n- **Machine operating conditions:** data of the equipment health over time (usually sensor-based and streaming). We will refer to this data as machine telemetry data.\n- **Error histor:** this data contains logs of non-breaking errors that happen thoughout a machine's operation and which parts of the machine they came from\n- **Failure history:** this data contains logs of severe errors that broke the machine down (requiring maintenance to operate again) and parts of the machine that caused it\n- **Maintenance/repair history:** what parts were fixed/replaced (as part of scheduled maintenance or due to failure) and when\n    Equipment metadata: anything we know about equipment and parts (such as make, model, etc.)"],"metadata":{}},{"cell_type":"code","source":["%run \"../includes/mnt_blob\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%sh ls /dbfs/mnt/data/telemetry"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["From now on we will adopt to following consistent terminology to avoid confusion:\n\n- A system as a whole will be called a **machine** and its parts are called **components**.\n- A machine can experience **errors** when anomalies happen. Errors do **NOT** result in shutdown, and they are **NOT** tied to any particular components, but they can cause one or several component to eventually fail.\n- A machine can experience **failure** when one of its components shuts down. This requires the component to be replaced before the machine can be operational again.\n- For our purposes, **maintenance** means a component was replaced. This can be either as part of a routine schedule or unscheduled maintenance (due to component failure)."],"metadata":{}},{"cell_type":"code","source":["%sh head /dbfs/mnt/data/telemetry/telemetry.csv"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["We can now start loading the data."],"metadata":{}},{"cell_type":"code","source":["df_telemetry = spark.read.csv(\"/mnt/data/telemetry/telemetry.csv\", header = \"true\", inferSchema = \"true\").cache()\ndf_failures = spark.read.csv(\"/mnt/data/telemetry/failures.csv\", header = \"true\", inferSchema = \"true\").cache()\ndf_errors = spark.read.csv(\"/mnt/data/telemetry/errors.csv\", header = \"true\", inferSchema = \"true\").cache()\ndf_maint = spark.read.csv(\"/mnt/data/telemetry/maintenance.csv\", header = \"true\", inferSchema = \"true\").cache()\ndf_machines = spark.read.csv(\"/mnt/data/telemetry/machines.csv\", header = \"true\", inferSchema = \"true\").cache()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Let's look at the schema to check our column types. Notice that the timestamp column has a `string` type. This willl need to later be changed to `timestamp` so we can perform date-time operations on it."],"metadata":{}},{"cell_type":"code","source":["df_telemetry.printSchema()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["We have four separate data sources, and as our first step toward data preparation, we combine all of them into one. We do so in this case by appending everything to the telemetry data using a left join. We then convert `datetime` into a `timestamp` column."],"metadata":{}},{"cell_type":"code","source":["keys = [\"machineID\", \"datetime\"]\n\ntype = \"left\"\n\ndf = df_telemetry.join(df_errors.withColumnRenamed(\"errorID\", \"error\"), keys, type) \\\n                 .join(df_failures.withColumnRenamed(\"failure\", \"fail\"), keys, type) \\\n                 .join(df_maint.withColumnRenamed(\"comp\", \"maint\"), keys, type) \\\n                 .join(df_machines, \"machineID\", type) \\\n                 .cache()\n\nfrom pyspark.sql.functions import to_timestamp\n\ndf = df.withColumn(\"datetime\", to_timestamp(\"datetime\", \"MM/dd/yyyy hh:mm:ss aa\")) \\\n       .orderBy(keys) \\\n       .dropDuplicates(keys)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Let's check our column types again."],"metadata":{}},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["We now create a temporary view from the DataFrame."],"metadata":{}},{"cell_type":"code","source":["df.createOrReplaceTempView(\"df\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Our first feature-engineering task will consist of getting rolling averages and standard deviations from the telemetries. Rolling statistics are calculated over a given time interval (or window), which we set at 3 hours. However, based on the use-case the window size can change."],"metadata":{}},{"cell_type":"markdown","source":["### Hands-on lab\n\nUse SQL to create a new table called `df_roll` that in addition to the original features also contains moving averages and standard deviations over a 3-hour window for the telemetry data (voltage, rotation, pressure and vibration). To do this you will need to use the `over` clause along with aggregate functions `mean` and `stddev`."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- put your solution here"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["%sql\n-- maximize this cell (click the + button on the right) to see the solution:\n\ndrop table if exists df_roll;\n\ncreate table df_roll as\nselect *,\n  mean(volt) over (partition by machineid order by datetime range between interval 3 hours preceding and current row) as volt_ma_3,\n  mean(rotate) over (partition by machineid order by datetime range between interval 3 hours preceding and current row) as rotate_ma_3,\n  mean(pressure) over (partition by machineid order by datetime range between interval 3 hours preceding and current row) as pressure_ma_3,\n  mean(vibration) over (partition by machineid order by datetime range between interval 3 hours preceding and current row) as vibration_ma_3,\n  stddev(volt) over (partition by machineid order by datetime range between interval 3 hours preceding and current row) as volt_sd_3,\n  stddev(rotate) over (partition by machineid order by datetime range between interval 3 hours preceding and current row) as rotate_sd_3,\n  stddev(pressure) over (partition by machineid order by datetime range between interval 3 hours preceding and current row) as pressure_sd_3,\n  stddev(vibration) over (partition by machineid order by datetime range between interval 3 hours preceding and current row) as vibration_sd_3\nfrom df \norder by machineID, datetime"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Create a time series plot comparing the original voltage telemetry to its 3-hour-window moving average, for machine with ID = 1. Does the moving average appear to be smoother compared to the original telemetry? How can we make it even smoother?"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- put your solution here"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["%sql\n-- maximize this cell (click the + button on the right) to see the solution:\nselect * from df_roll where machineID = 1 order by datetime;"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["### End of lab"],"metadata":{}},{"cell_type":"markdown","source":["In the above example, we saw how Spark SQL can be used to compute the rolling averages. For illustration, let's now do the same computation using the DataFrame APIs. This allows us to compare and contrast the two."],"metadata":{}},{"cell_type":"code","source":["# this is the alternative (probably better) way of calculating the rolling averages\n\nfrom pyspark.sql.functions import avg, stddev, col\nfrom pyspark.sql.window import Window\n\nw = (Window.partitionBy(col(\"machineID\")).orderBy(col(\"datetime\").cast('long')).rangeBetween(-3 * 60 * 60, 0))\n\ndf_roll = df.withColumn('volt_ma_3', avg(\"volt\").over(w)) \\\n            .withColumn('rotate_ma_3', avg(\"rotate\").over(w)) \\\n            .withColumn('pressure_ma_3', avg(\"pressure\").over(w)) \\\n            .withColumn('vibration_ma_3', avg(\"vibration\").over(w)) \\\n            .withColumn('volt_sd_3', stddev(\"volt\").over(w)) \\\n            .withColumn('rotate_sd_3', stddev(\"rotate\").over(w)) \\\n            .withColumn('pressure_sd_3', stddev(\"pressure\").over(w)) \\\n            .withColumn('vibration_sd_3', stddev(\"vibration\").over(w)) \\\n            .cache()\n\ndf_roll.createOrReplaceTempView(\"df_roll\")"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["To be safe, let's re-create the last time series plot to make sure that the two look similar."],"metadata":{}},{"cell_type":"code","source":["display(df_roll.filter(col(\"machineID\") == 1).orderBy(col(\"datetime\")))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Let's now look at the distribution of errors for each machine. This helps us understand the errors a little better. Recall that errors don't break the machine immediately, but can eventually lead to downtime. However, we must ensure that errors are relatively evenly distributed across machines otherwise our model may fail to generalize well."],"metadata":{}},{"cell_type":"code","source":["df1 = df_roll.select(\"machineID\", \"error\").filter(col(\"error\").isNull() == False).groupBy(\"machineID\", \"error\").count().orderBy(\"machineID\", \"error\")"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["display(df1)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["It's time for us to do some feature engineering with the error, failure and maintenance history. At a high level, our feature engineering will consist of three important steps:\n\n- Create one-hot-encoded features (i.e. \"dummy variables\") from `error`, `failure` and `maint`. This is an intermediate step because we will not use these features for modeling. Instead we use them to compute the following set of features.\n- Create features that measure for any timestamp how many hours have elapsed since the last time an error or a failure happened, and how many hours have elapsed since the last time maintenance was performed.\n- Use `failure` to create a target variable.\n\nLet's first take a look at how `error`, `failure` and `maint` break down:"],"metadata":{}},{"cell_type":"code","source":["df_roll.groupBy(\"error\").count().sort(\"count\").show()\ndf_roll.groupBy(\"fail\").count().sort(\"count\").show()\ndf_roll.groupBy(\"maint\").count().sort(\"count\").show()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["As we can see, failure and maintenance are recorded component by component. So for any machine, a given component can fail at a given time and eventually be replaced at some other time. On the other hand, error is NOT component by component but concerns the machine as a whole. So we know that a machine at a given time can experience one of 5 kinds of errors.\n\nTo get to our features, we first need to run one-hot encoding on the error, failure and maintenance columns. Prior to that we need to replace any missing values, which we encode here using the string \"zzzz\"."],"metadata":{}},{"cell_type":"code","source":["# we fill NAs for errors prior to one-hot-encoding, and use 'zzzzz' so we can sort alphabetically\ndf_roll = df_roll.fillna(\"zzzzz\", subset = \"error\") \\\n                 .fillna(\"zzzzz\", subset = [\"fail\", \"maint\"]) \\\n                 .drop(\"volt\").drop(\"rotate\").drop(\"pressure\").drop(\"vibration\")"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["We can now create a pipeline that will do one-hot encoding on the above features. Since they are of `string` type, we begin by using `StringIndexer` to first index the features and we then pass the results to `OneHotEncoderEstimator` which runs one-hot encoding on the features and stores the results as a **sparse matrix**, encoded using `vectortype`."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\nstages = []\ncat_cols = [\"error\", \"fail\", \"maint\"]\n\nfor cat_col in cat_cols:\n  indexer = StringIndexer(inputCol = cat_col, outputCol = cat_col + \"_index\", stringOrderType = \"alphabetAsc\")\n  encoder = OneHotEncoderEstimator(inputCols = [indexer.getOutputCol()], outputCols = [cat_col + \"_vec\"], dropLast = True)\n  stages += [indexer, encoder]\n  \npipeline = Pipeline(stages=stages)\ndf_encoded = pipeline.fit(df_roll).transform(df_roll)\ndf_encoded = df_encoded.drop(\"error\").drop(\"fail\").drop(\"maint\")\ndisplay(df_encoded)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["df_encoded.printSchema()"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["In order to calculate the time elapsed since the last error, failure and maintenance event, we first need to take our one-hot-encoded sparse vactor feature and break it up into indiviual binary features for each event."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import IntegerType\ncat_cols = [\"error\", \"fail\", \"maint\"]\n\nfor cc in cat_cols:\n  indexes = df_encoded.select(cc + '_index').distinct().rdd.flatMap(lambda x : x).collect()\n  indexes.sort()\n  print(indexes)\n  for index in indexes[:-1]: # we remove the last index\n      function = udf(lambda item: 1 if item == index else 0, IntegerType())\n      new_column_name = cc + '_' + str(int(index))\n      df_encoded = df_encoded.withColumn(new_column_name, function(col(cc + '_index')))"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["We can now drop the original sparse vector features."],"metadata":{}},{"cell_type":"code","source":["cols_drop = cat_cols + [c + \"_vec\" for c in cat_cols]\ndf_encoded = df_encoded.select([col for col in df_encoded.columns if col not in cols_drop])\ndisplay(df_encoded)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["Let's take a look at an example of one of the one-hot-encoded features we created: `error_4` is a binary feature that is set to 1 for any timestamp that corresponds to an error of type 4 in the machine (and 0 otherwise). We can see in the below result how many errors of type 4 occured in the overall data (across all machines)."],"metadata":{}},{"cell_type":"code","source":["df_encoded.groupBy(\"error_4\").count().show()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["We now use the binary features we created in the previous step to create a new set of features. If `error_4` is a binary feature that represents the an occurence of error of type 4 for a given machine and timestamp, we can use it to create a corresponding feature called `diff_error_4` which measures the number of hours elapsed since the last time an error of type 4 occured."],"metadata":{}},{"cell_type":"code","source":["# this is the alternative (probably better) way of calculating the rolling averages\n\nfrom pyspark.sql.functions import avg, lag, when, lit, sum, row_number\nfrom pyspark.sql.window import Window\n\nw = Window.partitionBy(col(\"machineID\")).orderBy(col(\"datetime\"))\n\n# first compute a lagged feature for datetime and convert both datetime and the lagged datetime to numbers\ndf_diff = df_encoded.withColumn('datetime_lag', lag(df_encoded['datetime']).over(w)) \\\n                    .withColumn('dt_long', col('datetime').cast('long')) \\\n                    .withColumn('dt_long_lag', col('datetime_lag').cast('long')) \\\n                    .drop('datetime_lag')\n\ncat_vars = [\"error\", \"fail\", \"maint\"]\n\nfor cat_var in cat_vars:\n  # find how many categories there are in each categorical feature\n  indexes = df_encoded.select(cat_var + '_index').distinct().rdd.flatMap(lambda x : x).collect()\n  indexes.sort()\n  for index in indexes[:-1]: # we remove the last category (these are the non-events)\n    cat_num = str(int(index))\n    cat_col = cat_var + '_' + cat_num\n    dif_col = 'd_' + cat_var + '_' + cat_num\n    cum_col = 'diff_' + cat_var + '_' + cat_num\n    print(cum_col)\n\n    # compute an intermediate feature used for grouping results\n    find_cum_grp = sum(col(cat_col)).over(w)\n    df_diff = df_diff.withColumn('cum_grp', find_cum_grp).cache()\n\n    # compute the number of hours elapsed since the last event (error, failure, or maintenance)\n    ww = Window.partitionBy(col(\"machineID\"), col(\"cum_grp\")).orderBy(col(\"datetime\"))\n    find_lag_diff = when(col(cat_col) == 1, lit(0)).otherwise((col('dt_long') - col('dt_long_lag')) / (60*60))\n    # for first occurence, we don't know when the last event was, so for now we just assume 100 hours\n    df_diff = df_diff.withColumn(dif_col, find_lag_diff) \\\n                     .fillna(100, subset = [dif_col]).cache()\n    \n    # reset the time elapsed back to 0 every time an event occurs (otherwise compute cumulative sum)\n    find_cum_diff = when(col(cat_col) == 1, lit(0)).otherwise(sum(col(dif_col)).over(ww))\n    df_diff = df_diff.withColumn(cum_col, find_cum_diff) \\\n                     .drop('cum_grp').drop(dif_col).drop(cat_col).cache() # if I don't use cache here I get a out-of-memory error\n  \n  df_diff = df_diff.drop(cat_var)\n\n\ndf_diff = df_diff.drop('dt_long').drop('dt_long_lag').orderBy('machineID', 'datetime')\ndisplay(df_diff)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["We can visualize our new features to get a better sense of what they look like. In this example, we look at the maintenance history. Recall that there are 4 components to each machine and maintenance happens component by component. In the plot below, components are represented by colors. As we move along the x-axis, we generally see a linear increase that represents time elapsing until there's a sharp drop back to 0 whenever maintenance is performed, at which point we reset the timer."],"metadata":{}},{"cell_type":"code","source":["display(df_diff.filter(col('machineID') == 1))"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["The last step in data prep is for us to create a targets for the PdM model. You might wonder why we don't just use `fail_0` through `fail_3` as our targets, since they indicate when a component failed (and hence the whole machine). In fact we could, but PdM is not about predicting when a machine fails, but predicting when it's **about to** fail. So it's better to create labels that indicate the state of the machine shortly **prior to failure** (how far back we want to go is something we need to determine use-case by use-case).\n\nAnother justification for picking the window size prior to failure is that it based on how long it takes to fix failures after they happen. If for example we expect a 3-day downtime, then we flag everything up to 3 days prior to failure."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nfrom pyspark.sql.functions import avg, lag, when, lit, sum\n\ndf_all = df_diff.cache()\n\nfor i in range(4): # iterate over the four components\n    # find all the times a component failed for a given machine\n    df_temp = df_diff.filter(col('fail_index') == i).select('machineID', 'datetime').toPandas()\n    label = 'y_' + str(i) # name of target column (one per component)\n    # create a new target column for each component and set to 0 by default\n    df_all = df_all.withColumn(label, lit(0))\n    for n in range(df_temp.shape[0]): # iterate over all the failure times\n        machineID, datetime = df_temp.iloc[n, :]\n        dt_end = datetime - pd.Timedelta('10 minutes') # from 10 minutes prior to failure\n        dt_start = datetime - pd.Timedelta('3 days') # up to 3 days prior to failure\n        if n % 500 == 0:\n            print(\"failure on machine {0} at {1}, so {2} is between {4} and {3}\".format(machineID, datetime, label, dt_end, dt_start))\n        # every time a failure happens, set the target feature to 1 within the provided window\n        find_labels = when((col('machineID') == int(machineID)) & (col('datetime').between(dt_start, dt_end)), lit(1)).otherwise(col(label))\n        df_all = df_all.withColumn(label, find_labels)\n        \ndisplay(df_all)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["As one example, we can visualize `y_0` by machine. Notice how rare our labels are."],"metadata":{}},{"cell_type":"code","source":["display(df_all.crosstab(\"machineID\", \"y_0\"))"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["The difficult parts of our data prep is over. Now we have just some house-cleaning to do before we hand the data over to the machine learning algorithms. Let's begin by checking if any missing data are present."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, sum, isnan\ndisplay(df_all.select(*(sum((col(c).isNull()).cast(\"int\")).alias(c) for c in df_all.columns)))"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":[""],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import Imputer\nimpute_cols = [c for c in df_all.columns if c.endswith('_sd_3')]\nprint(impute_cols)\nimputer = Imputer(inputCols = impute_cols, outputCols = impute_cols)\nmodel = imputer.fit(df_all)\n# model.surrogateDF.show()\ndf_all = model.transform(df_all)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["X_drop = ['error_index', 'fail_index', 'maint_index', 'f_1', 'f_2', 'f_3', 'f_4', 'y_0, ''y_1', 'y_2', 'y_3', 'model']\nY_keep = ['y_0', 'y_1', 'y_2', 'y_3']\nkeys = ['machineID', 'datetime']\n\nX_keep = list(set(df_all.columns) - set(X_drop + Y_keep + keys))\ndisplay(df_all.select(keys + sorted(X_keep + Y_keep)))"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["from pyspark.ml.feature import Normalizer, VectorAssembler\nfrom pyspark.ml import Pipeline\n\nvassembler = VectorAssembler(inputCols = X_keep, outputCol = \"features\")\nnormalizer = Normalizer(inputCol = \"features\", outputCol = \"norm_features\", p = 1.0)\n# df_norm = normalizer.transform(df_all)\n# display(df_norm)\n\npipeline = Pipeline(stages = [vassembler, normalizer])\ndf_norm = pipeline.fit(df_all).transform(df_all).select(keys + [\"norm_features\"] + Y_keep)\ndisplay(df_norm)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["If you wanted to, you could run the following code to split the ```norm_features``` vector up into separate columns.  This could be useful if you wanted to export the data into e.g. a CSV file."],"metadata":{}},{"cell_type":"code","source":["# from pyspark.sql.functions import udf, col\n# from pyspark.sql.types import ArrayType, DoubleType\n\n# n_features = 21\n\n# def to_array(col):\n#     def to_array_(v):\n#         return v.toArray().tolist()\n#     return udf(to_array_, ArrayType(DoubleType()))(col)\n\n# df = (df_norm\n#     .withColumn(\"xs\", to_array(col(\"norm_features\")))\n#     .select([\"machineID\", \"datetime\"] + [col(\"xs\")[i] for i in range(n_features)] + ['y_0', 'y_1', 'y_2', 'y_3']))\n"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["dbutils.fs.rm(\"dbfs:/FileStore/tables/preprocessed\", recurse = True)\n\ndf_norm.write.parquet(\"dbfs:/FileStore/tables/preprocessed\")"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."],"metadata":{}}],"metadata":{"name":"03_data_prep_lab","notebookId":3152348618211865},"nbformat":4,"nbformat_minor":0}
