{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preperation and feature engineering for PdM\n",
    "\n",
    "Over the course of the three days of this AI Airlift, we will develop and end-to-end solution for predictive maintenance.  The previous lab offered an introduction to ADB and DataFrames.  This lab applies, and builds on, this previous lab, to do the data preparation and feature engineering for [predictive maintenance](https://en.wikipedia.org/wiki/Predictive_maintenance).\n",
    "\n",
    "The relevant data sources for predictive maintenance (PdM) include, but are not limited to:\n",
    "- **Machine operating conditions:** data of the equipment health over time (usually sensor-based and streaming). We will refer to this data as machine telemetry data.\n",
    "- **Error history:** this data contains logs of non-breaking errors that happen thoughout a machine's operation and which parts of the machine they came from\n",
    "- **Failure history:** this data contains logs of severe errors that broke the machine down (requiring maintenance to operate again) and parts of the machine that caused it\n",
    "- **Maintenance/repair history:** what parts were fixed/replaced (as part of scheduled maintenance or due to failure) and when\n",
    "    Equipment metadata: anything we know about equipment and parts (such as make, model, etc.)\n",
    "    \n",
    "One important detail here is that we are dealing with time series data, because datapoints are collected once an hour.  Using time series data offers us an opportunity to apply some feature engineering methods unique to these kind of data.\n",
    "\n",
    "## Mount the data\n",
    "\n",
    "As usual, we start with mounting the data. At this point, the data are probably already mounted, because you ran previous labs. In this case, the next cell should run very fast and simply print \"Already mounted.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../includes/mnt_blob\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what files are stored in our predictive maintenance dataset. We can do this by envoking a command shell with the magic `%sh`, and running standard unix command for listing the contents of a directory.  Alternatively, we could use the `%fs` magic. Feel free to try it and observe the differences in usage and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh ls /dbfs/mnt/data/telemetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs ls /mnt/data/telemetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on we will adopt to following consistent terminology to avoid confusion:\n",
    "\n",
    "- A system as a whole will be called a **machine** and its parts are called **components**.\n",
    "- A machine can experience **errors** when anomalies happen. Errors do **NOT** result in shutdown, and they are **NOT** tied to any particular components, but they can cause one or several component to eventually fail.\n",
    "- A machine can experience **failure** when one of its components shuts down. This requires the component to be replaced before the machine can be operational again.\n",
    "- For our purposes, **maintenance** means a component was replaced. This can be either as part of a routine schedule or unscheduled maintenance (due to component failure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telemetry data\n",
    "\n",
    "Let's beginning by looking at the file format of our data. All data are stored in CSV files.  For example, `telemetry.csv` contains the data collected by four sensors on each machine (\"volt\", \"rotate\", \"pressure\", \"vibration\") once an hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh head /dbfs/mnt/data/telemetry/telemetry.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start loading the data into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telemetry = spark.read.csv(\"/mnt/data/telemetry/telemetry.csv\", header = \"true\", inferSchema = \"true\").cache()\n",
    "df_failures = spark.read.csv(\"/mnt/data/telemetry/failures.csv\", header = \"true\", inferSchema = \"true\").cache()\n",
    "df_errors = spark.read.csv(\"/mnt/data/telemetry/errors.csv\", header = \"true\", inferSchema = \"true\").cache()\n",
    "df_maint = spark.read.csv(\"/mnt/data/telemetry/maintenance.csv\", header = \"true\", inferSchema = \"true\").cache()\n",
    "df_machines = spark.read.csv(\"/mnt/data/telemetry/machines.csv\", header = \"true\", inferSchema = \"true\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the schema to check our column types. Notice that the timestamp column has a `string` type. This willl need to later be changed to `timestamp` so we can perform date-time operations on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telemetry.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have four separate data sources, and as our first step of data preparation, we combine all of them into one. We do so in this case by appending everything to the telemetry data using a left join. We then convert `datetime` into a `timestamp` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"machineID\", \"datetime\"]\n",
    "\n",
    "type = \"left\"\n",
    "\n",
    "df = df_telemetry.join(df_errors.withColumnRenamed(\"errorID\", \"error\"), keys, type) \\\n",
    "                 .join(df_failures.withColumnRenamed(\"failure\", \"fail\"), keys, type) \\\n",
    "                 .join(df_maint.withColumnRenamed(\"comp\", \"maint\"), keys, type) \\\n",
    "                 .join(df_machines, \"machineID\", type) \\\n",
    "                 .cache()\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "df = df.withColumn(\"datetime\", to_timestamp(\"datetime\", \"MM/dd/yyyy hh:mm:ss aa\")) \\\n",
    "       .orderBy(keys) \\\n",
    "       .dropDuplicates(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our column types again. Can you confirm that the time of data collection is encoded correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a temporary view from the DataFrame, which will allow us to run SQL commands on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Averages and Standard Deviations\n",
    "\n",
    "Our first feature-engineering task will consist of getting rolling averages and standard deviations from the telemetries. Rolling statistics are calculated over a given time interval (or window), which we set at 3 hours. However, based on the use-case the window size can change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on lab\n",
    "\n",
    "Use SQL to create a new table called `df_roll` that in addition to the original features also contains moving averages and standard deviations over a 3-hour window for the telemetry data (voltage, rotation, pressure and vibration). To do this you will need to use the `over` clause along with aggregate functions `mean` and `stddev`.\n",
    "\n",
    "To do this, you will have to `partition by` *machineID*, make sure to `order by` *datetime*, and operate over an intervall of 3 hours.\n",
    "\n",
    "Here is an example of how to calculate how to calculate a running average over a 3-hour time window for the voltage sensor on machine 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \n",
    "\n",
    "drop table if exists df_roll;\n",
    "\n",
    "create table df_roll as\n",
    "select *,\n",
    "  mean(volt) over (order by datetime range between interval 3 hours preceding and current row) as volt_ma_3\n",
    "from df\n",
    "where machineid == 1\n",
    "order by datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create your solution, you have to change the code so that it executes over all machines. Then add lines for doing the same for `stddev`, as well as the other sensors.\n",
    "\n",
    "> Hint: While you are iteratively developing your solution, it may be useful to delete the results of previous attempts.  You can do this by adding the line `drop table if exists df_roll;` towards the top of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- put your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- maximize this cell (click the + button on the right) to see the solution:\n",
    "\n",
    "drop table if exists df_roll;\n",
    "\n",
    "create table df_roll as\n",
    "select *,\n",
    "  mean(volt) over (partition by machineid order by datetime range between interval 3 hours preceding and current row) as volt_ma_3,\n",
    "  mean(rotate) over (partition by machineid order by datetime range between interval 3 hours preceding and current row) as rotate_ma_3,\n",
    "  mean(pressure) over (partition by machineid order by datetime range between interval 3 hours preceding and current row) as pressure_ma_3,\n",
    "  mean(vibration) over (partition by machineid order by datetime range between interval 3 hours preceding and current row) as vibration_ma_3,\n",
    "  stddev(volt) over (partition by machineid order by datetime range between interval 3 hours preceding and current row) as volt_sd_3,\n",
    "  stddev(rotate) over (partition by machineid order by datetime range between interval 3 hours preceding and current row) as rotate_sd_3,\n",
    "  stddev(pressure) over (partition by machineid order by datetime range between interval 3 hours preceding and current row) as pressure_sd_3,\n",
    "  stddev(vibration) over (partition by machineid order by datetime range between interval 3 hours preceding and current row) as vibration_sd_3\n",
    "from df \n",
    "order by machineID, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a time series plot comparing the original voltage telemetry to its 3-hour-window moving average, for machine with ID = 1. Does the moving average appear to be smoother compared to the original telemetry? How can we make it even smoother?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- put your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- maximize this cell (click the + button on the right) to see the solution:\n",
    "select * from df_roll where machineID = 1 order by datetime;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we saw how Spark SQL can be used to compute the rolling averages. For illustration, let's now do the same computation using the DataFrame APIs. This allows us to compare and contrast the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the alternative (probably better) way of calculating the rolling averages\n",
    "\n",
    "from pyspark.sql.functions import avg, stddev, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = (Window.partitionBy(col(\"machineID\")).orderBy(col(\"datetime\").cast('long')).rangeBetween(-3 * 60 * 60, 0))\n",
    "\n",
    "df_roll = df.withColumn('volt_ma_3', avg(\"volt\").over(w)) \\\n",
    "            .withColumn('rotate_ma_3', avg(\"rotate\").over(w)) \\\n",
    "            .withColumn('pressure_ma_3', avg(\"pressure\").over(w)) \\\n",
    "            .withColumn('vibration_ma_3', avg(\"vibration\").over(w)) \\\n",
    "            .withColumn('volt_sd_3', stddev(\"volt\").over(w)) \\\n",
    "            .withColumn('rotate_sd_3', stddev(\"rotate\").over(w)) \\\n",
    "            .withColumn('pressure_sd_3', stddev(\"pressure\").over(w)) \\\n",
    "            .withColumn('vibration_sd_3', stddev(\"vibration\").over(w)) \\\n",
    "            .cache()\n",
    "\n",
    "df_roll.createOrReplaceTempView(\"df_roll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be safe, let's re-create the last time series plot to make sure that the two look similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_roll.filter(col(\"machineID\") == 1).orderBy(col(\"datetime\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the distribution of errors for each machine. This helps us understand the errors a little better. Recall that errors don't break the machine immediately, but can eventually lead to downtime. However, we must ensure that errors are relatively evenly distributed across machines otherwise our model may fail to generalize well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_roll.select(\"machineID\", \"error\").filter(col(\"error\").isNull() == False).groupBy(\"machineID\", \"error\").count().orderBy(\"machineID\", \"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good news, the all error types are distributed across all machines.\n",
    "\n",
    "## Feature engineering for error, failure and maintenance data\n",
    "\n",
    "It's time for us to do some feature engineering with the error, failure and maintenance history. At a high level, our feature engineering will consist of three important steps:\n",
    "\n",
    "- Create one-hot-encoded features (i.e. \"dummy variables\") from `error`, `failure` and `maint`. This is an intermediate step because we will not use these features for modeling. Instead we use them to compute the following set of features.\n",
    "- Create features that measure for any timestamp how many hours have elapsed since the last time an error or a failure happened, and how many hours have elapsed since the last time maintenance was performed.\n",
    "- Use `failure` to create a target variable.\n",
    "\n",
    "Let's first take a look at how `error`, `failure` and `maint` break down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_roll.groupBy(\"error\").count().sort(\"count\").show()\n",
    "df_roll.groupBy(\"fail\").count().sort(\"count\").show()\n",
    "df_roll.groupBy(\"maint\").count().sort(\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, failure and maintenance are recorded component by component. So for any machine, a given component can fail at a given time and eventually be replaced at some other time. On the other hand, error is NOT component by component but concerns the machine as a whole. So we know that a machine at a given time can experience one of 5 kinds of errors.\n",
    "\n",
    "To get to our features, we first need to run one-hot encoding on the error, failure and maintenance columns. Prior to that we need to replace any missing values, which we encode here using the string \"zzzz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we fill NAs for errors prior to one-hot-encoding, and use 'zzzzz' so we can sort alphabetically\n",
    "df_roll = df_roll.fillna(\"zzzzz\", subset = [\"error\", \"fail\", \"maint\"]) \\\n",
    "                 .drop(\"volt\", \"rotate\",\"pressure\",\"vibration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a pipeline that will do one-hot encoding on the above features. Since they are of `string` type, we begin by using `StringIndexer` to first index the features and we then pass the results to `OneHotEncoderEstimator` which runs one-hot encoding on the features and stores the results as a **sparse matrix**, encoded using `vectortype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\n",
    "stages = []\n",
    "cat_cols = [\"error\", \"fail\", \"maint\"]\n",
    "\n",
    "for cat_col in cat_cols:\n",
    "  indexer = StringIndexer(inputCol = cat_col, outputCol = cat_col + \"_index\", stringOrderType = \"alphabetAsc\")\n",
    "  encoder = OneHotEncoderEstimator(inputCols = [indexer.getOutputCol()], outputCols = [cat_col + \"_vec\"], dropLast = True)\n",
    "  stages += [indexer, encoder]\n",
    "  \n",
    "pipeline = Pipeline(stages=stages)\n",
    "df_encoded = pipeline.fit(df_roll).transform(df_roll)\n",
    "df_encoded = df_encoded.drop(\"error\").drop(\"fail\").drop(\"maint\")\n",
    "display(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the time elapsed since the last error, failure and maintenance event, we first need to take our one-hot-encoded sparse vactor feature and break it up into indiviual binary features for each event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType\n",
    "cat_cols = [\"error\", \"fail\", \"maint\"]\n",
    "\n",
    "for cc in cat_cols:\n",
    "  indexes = df_encoded.select(cc + '_index').distinct().rdd.flatMap(lambda x : x).collect()\n",
    "  indexes.sort()\n",
    "  print(indexes)\n",
    "  for index in indexes[:-1]: # we remove the last index\n",
    "      function = udf(lambda item: 1 if item == index else 0, IntegerType())\n",
    "      new_column_name = cc + '_' + str(int(index))\n",
    "      df_encoded = df_encoded.withColumn(new_column_name, function(col(cc + '_index')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now drop the original sparse vector features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = cat_cols + [c + \"_vec\" for c in cat_cols]\n",
    "df_encoded = df_encoded.select([col for col in df_encoded.columns if col not in cols_drop])\n",
    "display(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at an example of one of the one-hot-encoded features we created: `error_4` is a binary feature that is set to 1 for any timestamp that corresponds to an error of type 4 in the machine (and 0 otherwise). We can see in the below result how many errors of type 4 occured in the overall data (across all machines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.groupBy(\"error_4\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the binary features we created in the previous step to create a new set of features. If `error_4` is a binary feature that represents the an occurence of error of type 4 for a given machine and timestamp, we can use it to create a corresponding feature called `diff_error_4` which measures the number of hours elapsed since the last time an error of type 4 occured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, lag, when, lit, sum, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window.partitionBy(col(\"machineID\")).orderBy(col(\"datetime\"))\n",
    "\n",
    "# first compute a lagged feature for datetime and convert both datetime and the lagged datetime to numbers\n",
    "df_diff = df_encoded.withColumn('datetime_lag', lag(df_encoded['datetime']).over(w)) \\\n",
    "                    .withColumn('dt_long', col('datetime').cast('long')) \\\n",
    "                    .withColumn('dt_long_lag', col('datetime_lag').cast('long')) \\\n",
    "                    .drop('datetime_lag')\n",
    "\n",
    "cat_vars = [\"error\", \"fail\", \"maint\"]\n",
    "\n",
    "for cat_var in cat_vars:\n",
    "  # find how many categories there are in each categorical feature\n",
    "  indexes = df_encoded.select(cat_var + '_index').distinct().rdd.flatMap(lambda x : x).collect()\n",
    "  indexes.sort()\n",
    "  for index in indexes[:-1]: # we remove the last category (these are the non-events)\n",
    "    cat_num = str(int(index))\n",
    "    cat_col = cat_var + '_' + cat_num\n",
    "    dif_col = 'd_' + cat_var + '_' + cat_num\n",
    "    cum_col = 'diff_' + cat_var + '_' + cat_num\n",
    "    print(cum_col)\n",
    "\n",
    "    # compute an intermediate feature used for grouping results\n",
    "    find_cum_grp = sum(col(cat_col)).over(w)\n",
    "    df_diff = df_diff.withColumn('cum_grp', find_cum_grp).cache()\n",
    "\n",
    "    # compute the number of hours elapsed since the last event (error, failure, or maintenance)\n",
    "    ww = Window.partitionBy(col(\"machineID\"), col(\"cum_grp\")).orderBy(col(\"datetime\"))\n",
    "    find_lag_diff = when(col(cat_col) == 1, lit(0)).otherwise((col('dt_long') - col('dt_long_lag')) / (60*60))\n",
    "    # for first occurence, we don't know when the last event was, so for now we just assume 100 hours\n",
    "    df_diff = df_diff.withColumn(dif_col, find_lag_diff) \\\n",
    "                     .fillna(100, subset = [dif_col]).cache()\n",
    "    \n",
    "    # reset the time elapsed back to 0 every time an event occurs (otherwise compute cumulative sum)\n",
    "    find_cum_diff = when(col(cat_col) == 1, lit(0)).otherwise(sum(col(dif_col)).over(ww))\n",
    "    df_diff = df_diff.withColumn(cum_col, find_cum_diff) \\\n",
    "                     .drop('cum_grp').drop(dif_col).drop(cat_col).cache() # if I don't use cache here I get a out-of-memory error\n",
    "  \n",
    "  df_diff = df_diff.drop(cat_var)\n",
    "\n",
    "\n",
    "df_diff = df_diff.drop('dt_long').drop('dt_long_lag').orderBy('machineID', 'datetime')\n",
    "display(df_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize our new features to get a better sense of what they look like. In this example, we look at the maintenance history. Recall that there are 4 components to each machine and maintenance happens component by component. In the plot below, components are represented by colors. As we move along the x-axis, we generally see a linear increase that represents time elapsing until there's a sharp drop back to 0 whenever maintenance is performed, at which point we reset the timer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_diff.filter(col('machineID') == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step in data prep is for us to create a targets for the PdM model. You might wonder why we don't just use `fail_0` through `fail_3` as our targets, since they indicate when a component failed (and hence the whole machine). In fact we could, but PdM is not about predicting when a machine fails, but predicting when it's **about to** fail. So it's better to create labels that indicate the state of the machine shortly **prior to failure** (how far back we want to go is something we need to determine use-case by use-case).\n",
    "\n",
    "Another justification for picking the window size prior to failure is that it based on how long it takes to fix failures after they happen. If for example we expect a 3-day downtime, then we flag everything up to 3 days prior to failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import avg, lag, when, lit, sum\n",
    "\n",
    "df_all = df_diff.cache()\n",
    "\n",
    "for i in range(4): # iterate over the four components\n",
    "    # find all the times a component failed for a given machine\n",
    "    df_temp = df_diff.filter(col('fail_index') == i).select('machineID', 'datetime').toPandas()\n",
    "    label = 'y_' + str(i) # name of target column (one per component)\n",
    "    # create a new target column for each component and set to 0 by default\n",
    "    df_all = df_all.withColumn(label, lit(0))\n",
    "    for n in range(df_temp.shape[0]): # iterate over all the failure times\n",
    "        machineID, datetime = df_temp.iloc[n, :]\n",
    "        dt_end = datetime - pd.Timedelta('10 minutes') # from 10 minutes prior to failure\n",
    "        dt_start = datetime - pd.Timedelta('3 days') # up to 3 days prior to failure\n",
    "        if n % 500 == 0:\n",
    "            print(\"failure on machine {0} at {1}, so {2} is between {4} and {3}\".format(machineID, datetime, label, dt_end, dt_start))\n",
    "        # every time a failure happens, set the target feature to 1 within the provided window\n",
    "        find_labels = when((col('machineID') == int(machineID)) & (col('datetime').between(dt_start, dt_end)), lit(1)).otherwise(col(label))\n",
    "        df_all = df_all.withColumn(label, find_labels)\n",
    "        \n",
    "display(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one example, we can visualize `y_0` by machine. Notice how rare our labels are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_all.crosstab(\"machineID\", \"y_0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difficult parts of our data prep is over. Now we have just some house-cleaning to do before we hand the data over to the machine learning algorithms. Let's begin by checking if any missing data are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, isnan\n",
    "display(df_all.select(*(sum((col(c).isNull()).cast(\"int\")).alias(c) for c in df_all.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "impute_cols = [c for c in df_all.columns if c.endswith('_sd_3')]\n",
    "print(impute_cols)\n",
    "imputer = Imputer(inputCols = impute_cols, outputCols = impute_cols)\n",
    "model = imputer.fit(df_all)\n",
    "# model.surrogateDF.show()\n",
    "df_all = model.transform(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_drop = ['error_index', 'fail_index', 'maint_index', 'f_1', 'f_2', 'f_3', 'f_4', 'y_0, ''y_1', 'y_2', 'y_3', 'model']\n",
    "Y_keep = ['y_0', 'y_1', 'y_2', 'y_3']\n",
    "keys = ['machineID', 'datetime']\n",
    "\n",
    "X_keep = list(set(df_all.columns) - set(X_drop + Y_keep + keys))\n",
    "display(df_all.select(keys + sorted(X_keep + Y_keep)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "vassembler = VectorAssembler(inputCols = X_keep, outputCol = \"features\")\n",
    "stndscaler = StandardScaler(inputCol = \"features\", outputCol = \"norm_features\")\n",
    "# df_norm = stndscaler.transform(df_all)\n",
    "# display(df_norm)\n",
    "\n",
    "pipeline = Pipeline(stages = [vassembler, stndscaler])\n",
    "df_norm = pipeline.fit(df_all).transform(df_all).select(keys + [\"norm_features\"] + Y_keep)\n",
    "display(df_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to, you could run the following code to split the ```norm_features``` vector up into separate columns.  This could be useful if you wanted to export the data into e.g. a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import udf, col\n",
    "# from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "# n_features = 21\n",
    "\n",
    "# def to_array(col):\n",
    "#     def to_array_(v):\n",
    "#         return v.toArray().tolist()\n",
    "#     return udf(to_array_, ArrayType(DoubleType()))(col)\n",
    "\n",
    "# df = (df_norm\n",
    "#     .withColumn(\"xs\", to_array(col(\"norm_features\")))\n",
    "#     .select([\"machineID\", \"datetime\"] + [col(\"xs\")[i] for i in range(n_features)] + ['y_0', 'y_1', 'y_2', 'y_3']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"dbfs:/FileStore/tables/preprocessed\", recurse = True)\n",
    "\n",
    "df_norm.write.parquet(\"dbfs:/FileStore/tables/preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  }
 ],
 "metadata": {
  "name": "03_data_prep_lab",
  "notebookId": 4057188818416105
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
