{"cells":[{"cell_type":"markdown","source":["#Introduction to DataFrames\n\nThis hands-on lab offers an introduction to Azure Databricks (ADB), with a focus DataFrames. You will learn best practices for Data Prep w/ ADB, by building a data preprocessing pipeline with essential transformations and estimators on DataFrames.\n\nLearning Goals:\n* Knowing how to mount your data in Azure Blob storage.\n* Understanding the relationship between between DataFrames, Datasets, and RDDs.\n* Using *actions*, such as `show()`, `display()`, and `count()`.\n* Using the *transformations*, such as `limit()`, `select()`, and `drop()`.\n* Understand the difference between `actions` and `transformations`.\n* Know how to find documentation on Spark.\n* Do some basic data cleansing and description.\n* Select and drop columns of your data.\n* Performing conversion between SQL and dataframes."],"metadata":{}},{"cell_type":"markdown","source":["## Mount Azure Blob storage in Azure Databricks\n\nFor this and most other labs, we stored the data in [Azure Blob storage](https://azure.microsoft.com/en-au/services/storage/blobs/).\n\nThere are two things you can take away from how we mount the data:\n1. The next cell demonstrates how to run another notebook from this notebook.  This can be very useful for creating [Notebook Workflows](https://docs.databricks.com/user-guide/notebooks/notebook-workflows.html). Here this other notebook is stored in the subdirectory `includes` in the parent directory to the present notebook.\n1. You can learn about how to configure `Shared Access Signatures` (SAS) for providing secure access to data stored in Azure Blob storage. See the databricks [documentation](https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html#mount-azure-blob-storage-containers-with-dbfs) for more details.\n\nIf you are curious, look at the contents of the notebook `mnt_blob` to see what happens there."],"metadata":{}},{"cell_type":"code","source":["%run \"../includes/mnt_blob\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## The Data Source\n\nThe data for this notebook are stored in a compressed [parquet](https://en.wikipedia.org/wiki/Apache_Parquet) \"file\" called **pagecounts** (~23 MB file from Wikipedia):\n* One hour of page counts from the English Wikimedia projects captured August 5, 2016, at 12:00 PM UTC.\n* Size on Disk: ~23 MB\n* Type: Compressed Parquet File\n* More Info: <a href=\"https://dumps.wikimedia.org/other/pagecounts-raw\" target=\"_blank\">Page view statistics for Wikimedia projects</a>\n\n\nWe can use the cell magic `%fs` to investigate the folder/file structure of the parquet file.  In general `%fs` allows you to use *dbutils* filesystem commands. For more information, see [Access DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html#dbfs-dbutils) with dbutils.\n\nThere are other magic commands for [mixing languages](https://docs.databricks.com/user-guide/notebooks/notebook-use.html#mix-languages) within a notebook."],"metadata":{}},{"cell_type":"code","source":["%fs ls /mnt/data/wikipedia/pagecounts/staging_parquet_en_only_clean/"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## Create a DataFrame\n\nLet's read the Parquet file into a `DataFrame`.\n\n* We'll start with the `spark` object, an instance of the [SparkSession](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/SparkSession.html) class and the entry point to Spark applications.\n* From there we can access the `read` object which gives us an instance of the class [DataFrameReader](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html).  This class offeres different methods for different types of data, such as `csv`, `json`, and `parquet`, which we use here.\n\nClosely look at the output of running this cell of code. Can you see the type of object that is returned? What columns are in the data, and what are the data types of each column?"],"metadata":{}},{"cell_type":"code","source":["parquetDir = \"/mnt/data/wikipedia/pagecounts/staging_parquet_en_only_clean/\"\n\npagecountsEnAllDF = (spark  # Our SparkSession & Entry Point\n  .read                     # Our DataFrameReader\n  .parquet(parquetDir)      # Returns an instance of DataFrame\n)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Actions and Transformations\n\nOn first sights, the distrinction between actions and transformations might be confusing. \n\n[Transformations](https://databricks.com/glossary/what-are-transformations) instruct Spark how you would like to modify the DataFrame you have into the one that you want.  The key thing to understand about transformations is that they don't actually do the transformation at the time they're specified. They describe a transformation that will be done. Another thing to understand is that you can \"pile up\" transformations, one after the other.\n\n*Actions* are operations that are executed immediately. Actions are often taken after a transformation, or sequences of transformations, to show the results of the transformations.\n\nLet's meet a couple of actions first.\n\n### count()\n\n`count()` will trigger a job to process the request and return a value.\n\nLet's use `count()` to count all records in our `DataFrame`."],"metadata":{}},{"cell_type":"code","source":["total = pagecountsEnAllDF.count()\n\nprint(\"Record Count: {0:,}\".format( total ))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["That tells us that there are around 2 million rows in the `DataFrame`."],"metadata":{}},{"cell_type":"markdown","source":["## cache()\n\nBefore we take a closer look at the contents of the `DataFrame`, let us introduce a technique that speeds up processing.\n\nThe ability to cache data is one technique for achieving better performance with Apache Spark. \n\nThis is because every action requires Spark to read the data from its source (Azure Blob, Amazon S3, HDFS, etc.) but caching moves that data into the memory of the local executor for \"instant\" access.\n\n>`persist()` is an alias for `cache()`. Both can be used to achieve identical results.\n\nLet's demonstrate this by running the action `count` twice, but including a `cache` action of the `DataFrame` during the first run. This should allow us to execute the `count` action much faster the second time."],"metadata":{}},{"cell_type":"code","source":["(pagecountsEnAllDF\n  .cache()         # Mark the DataFrame as cached\n  .count()         # Materialize the cache\n) "],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["If you re-run that command, it should take significantly less time."],"metadata":{}},{"cell_type":"code","source":["pagecountsEnAllDF.count()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["And as a quick side note, you can remove a cache by calling the `DataFrame`'s `unpersist()` method but, it is not necessary."],"metadata":{}},{"cell_type":"code","source":["pagecountsEnAllDF.unpersist()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Now the `count` action should take longer.\n\n> Note that it doesn't take as long if we don't also `cache` the `DataFrame` in the process."],"metadata":{}},{"cell_type":"code","source":["pagecountsEnAllDF.count()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Our Data\n\nLet's continue by taking a look at the type of data we have. \n\nWe can do this with the `printSchema()` command:"],"metadata":{}},{"cell_type":"code","source":["pagecountsEnAllDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["We can now see that we have four columns of data:\n* **project** (*string*): The name of the Wikipedia project. This will include values such as:\n  * **en**: The English version of Wikipedia.\n  * **fr**: The French version of Wikipedia.\n  * **en.d**: The English version of Wiktionary.\n  * **fr.b**: The French version of Wikibooks.\n  * **de.n**: The German version of Wikinews.\n* **article** (*string*): The name of the article in the corresponding project. This will include values such as:\n  * <a href=\"https://en.wikipedia.org/wiki/Apache_Spark\" target=\"_blank\">Apache_Spark</a>\n  * <a href=\"https://en.wikipedia.org/wiki/Matei_Zaharia\" target=\"_blank\">Matei_Zaharia</a>\n  * <a href=\"https://en.wikipedia.org/wiki/Kevin_Bacon\" target=\"_blank\">Kevin_Bacon</a>\n* **requests** (*integer*): The number of requests (clicks) the article has received in the hour this data represents.\n* **bytes_served** (*long*): The total number of bytes delivered for the requested article.\n  * **Note:** In our copy of the data, this value is zero for all records and consequently is of no value to us."],"metadata":{}},{"cell_type":"markdown","source":["## Spark API Documentation\n\n\nTry to find the documentation for `count()`.  Hint: There are two ways to find the documentation on this action:\n- Go to the online Spark API  documentation for [printSchema](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=printSchema#pyspark.sql.DataFrame.printSchema).\n- Create a new cell below with the code `help(pagecountsEnAllDF.printSchema)` and execute the cell.\n\nYou have already seen one command available to the `DataFrame` class, namely `DataFrame.printSchema()`\n  \nLet's take a look at the API to see what other operations we have available."],"metadata":{}},{"cell_type":"markdown","source":["### **Spark API Home Page**\n0. Open a new browser tab\n0. Google for **Spark API Latest** or **Spark API _x.x.x_** for a specific version.\n0. Select **Spark API Documentation - Spark _x.x.x_ Documentation - Apache Spark** \n\nOther Documentation:\n* Programming Guides for DataFrames, SQL, Graphs, Machine Learning, Streaming...\n* Deployment Guides for Spark Standalone, Mesos, Yarn...\n* Configuration, Monitoring, Tuning, Security...\n\nHere are some shortcuts\n  * <a href=\"https://spark.apache.org/docs/latest/api.html\" target=\"_blank\">Spark API Documentation - Latest</a>\n  * <a href=\"https://spark.apache.org/docs/2.1.1/api.html\" target=\"_blank\">Spark API Documentation - 2.1.1</a>\n  * <a href=\"https://spark.apache.org/docs/2.1.0/api.html\" target=\"_blank\">Spark API Documentation - 2.1.0</a>\n  * <a href=\"https://spark.apache.org/docs/2.0.2/api.html\" target=\"_blank\">Spark API Documentation - 2.0.2</a>\n  * <a href=\"https://spark.apache.org/docs/1.6.3/api.html\" target=\"_blank\">Spark API Documentation - 1.6.3</a>"],"metadata":{}},{"cell_type":"markdown","source":["Naturally, which set of documentation you will use depends on which language you will use."],"metadata":{}},{"cell_type":"markdown","source":["### Spark API (Python)\n\n0. Select **Spark Python API (Sphinx)**.\n0. Look up the documentation for `pyspark.sql.DataFrame`.\n  0. In the lower-left-hand-corner type **DataFrame** into the search field.\n  0. Hit **[Enter]**.\n  0. The search results should appear in the right-hand pane.\n  0. Click on **pyspark.sql.DataFrame (Python class, in pyspark.sql module)**\n  0. The documentation should open in the right-hand pane."],"metadata":{}},{"cell_type":"markdown","source":["### Spark API (Scala)\n\n0. Select **Spark Scala API (Scaladoc)**.\n0. Look up the documentation for `org.apache.spark.sql.DataFrame`.\n  0. In the upper-left-hand-corner type **DataFrame** into the search field.\n  0. The search will execute automatically.\n  0. In the class/package list, click on **DataFrame**.\n  0. The documentation should open in the right-hand pane.\n  \nThis isn't going to work, but why?"],"metadata":{}},{"cell_type":"markdown","source":["### Spark API (Scala), Try #2\n\nLook up the documentation for `org.apache.spark.sql.Dataset`.\n  0. In the upper-left-hand-corner type **Dataset** into the search field.\n  0. The search will execute automatically.\n  0. In the class/package list, click on **Dataset**.\n  0. The documentation should open in the right-hand pane."],"metadata":{}},{"cell_type":"markdown","source":["Now that we have found the proper documentation, we can take a quick peek at the function `printSchema()`.\n\nNothing special here.\n\nIf you look at the API docs, `printSchema(..)` is described like this:\n> Prints the schema to the console in a nice tree format."],"metadata":{}},{"cell_type":"markdown","source":["## show(..)\n\nWhat we want to look for next is a function that will allow us to print the data to the console.\n\nIn the API docs for `DataFrame`/`Dataset` find the docs for the `show(..)` command(s).\n\nIn the case of Python, we have one method with two optional parameters.<br/>\nIn the case of Scala, we have several overloaded methods.<br/>\n\nIn either case, the `show(..)` method effectively has two optional parameters:\n* **n**: The number of records to print to the console, the default being 20.\n* **truncate**: If true, columns wider than 20 characters will be truncated, where the default is true.\n\nLet's take a look at the data in our `DataFrame` with the `show()` command:"],"metadata":{}},{"cell_type":"code","source":["pagecountsEnAllDF.show()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["In the cell above, change the parameters of the show command to:\n* print only the first five records\n* disable truncation\n* print only the first ten records and disable truncation\n\n**Note:** The function `show(..)` is an **action** which triggers a job."],"metadata":{}},{"cell_type":"markdown","source":["## display(..)\n\nThe `show(..)` command is part of the core Spark API and simply prints the results to the console.\n\nOur notebooks have a slightly more elegant alternative.\n\nInstead of calling `show(..)` on an existing `DataFrame` we can instead pass our `DataFrame` to the `display(..)` command:"],"metadata":{}},{"cell_type":"code","source":["display(pagecountsEnAllDF)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["### show(..) vs display(..)\n* `show(..)` is part of core spark - `display(..)` is specific to our notebooks.\n* `show(..)` is ugly - `display(..)` is pretty.\n* `show(..)` has parameters for truncating both columns and rows - `display(..)` does not.\n* `show(..)` is a function of the `DataFrame`/`Dataset` class - `display(..)` works with a number of different objects.\n* `display(..)` is more powerful - with it, you can...\n  * Download the results as CSV\n  * Render line charts, bar chart & other graphs, maps and more.\n  * See up to 1000 records at a time.\n  \nFor the most part, the difference between the two is going to come down to preference.\n\nLike `DataFrame.show(..)`, `display(..)` is an **action** which triggers a job."],"metadata":{}},{"cell_type":"markdown","source":["## Transformations\n\nBoth `show(..)` and `display(..)` are **actions** that trigger jobs (though in slightly different ways).\n\nIf you recall, `show(..)` has a parameter to control how many records are printed but, `display(..)` does not.\n\nWe can address that difference with our first *transformation*, `limit(..)`.\n\n### limit(..)\n\nIf you look at the API docs, `limit(..)` is described like this:\n> Returns a new Dataset by taking the first n rows...\n\n`show(..)`, like many actions, does not return anything. \n\nOn the other hand, transformations like `limit(..)` return a **new** `DataFrame`:"],"metadata":{}},{"cell_type":"code","source":["limitedDF = pagecountsEnAllDF.limit(5) # \"limit\" the number of records to the first 5"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["Notice how \"nothing\" happened - that is no job was triggered.\n\nThis is because we are simply defining the second step in our transformations.\n  1. Read in the parquet file (represented by **pagecountsEnAllDF**).\n  1. Limit those records to just the first 5 (represented by **limitedDF**).\n\nIt's not until we induce an action that a job is triggered and the data is processed\n\nWe can to this with either the `show(..)` or the `display(..)` actions.\n\nFor example, we can `show` the first 100 rows of the DataFrame `limitedDF`, which only has 5 row."],"metadata":{}},{"cell_type":"code","source":["limitedDF.show(100, False) #show up to 100 records and don't truncate the columns"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["We can use `display` to achieve a similar, but prettier result."],"metadata":{}},{"cell_type":"code","source":["display(limitedDF) # defaults to the first 1000 records"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) select(..)\n\nLet's say, for the sake of argument, that we don't want to look at all the data:"],"metadata":{}},{"cell_type":"code","source":["pagecountsEnAllDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["For example, it was asserted above that **bytes_served** had nothing but zeros in it and consequently is of no value to us.\n\nIf that is the case, we can disregard it by selecting only the three columns that we want:"],"metadata":{}},{"cell_type":"code","source":["# Transform the data by selecting only three columns\nonlyThreeDF = (pagecountsEnAllDF\n  .select(\"project\", \"article\", \"requests\") # Our 2nd transformation (4 >> 3 columns)\n)\n# Now let's take a look at what the schema looks like\nonlyThreeDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["Again, notice how the call to `select(..)` does not trigger a job.\n\nThat's because `select(..)` is a transformation. It's just one more step in a long list of transformations.\n\nLet's go ahead and invoke the action `show(..)` and take a look at the result."],"metadata":{}},{"cell_type":"code","source":["# And lastly, show the first five records which should exclude the bytes_served column.\nonlyThreeDF.show(5, False)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["The `select(..)` command is one of the most powerful and most commonly used transformations. \n\nWe will see plenty of other examples of its usage as we progress.\n\nIf you look at the API docs, `select(..)` is described like this:\n> Returns a new Dataset by computing the given Column expression for each element.\n\nThe \"Column expression\" referred to there is where the true power of this operation shows up. Again, we will go deeper on these later.\n\nJust like `limit(..)`, `select(..)` \n* does not trigger a job\n* returns a new `DataFrame`\n* simply defines the next transformation in a sequence of transformations."],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) drop(..)\n\nAs a quick side note, you will quickly discover there are a lot of ways to accomplish the same task.\n\nTake the transformation `drop(..)` for example - instead of selecting everything we wanted, `drop(..)` allows us to specify the columns we don't want.\n\nIf you look at the API docs, `drop(..)` is described like this:\n> Returns a new Dataset with a column dropped.\n\nAnd we can see that we can produce the same result as the last exercise this way:"],"metadata":{}},{"cell_type":"code","source":["# Transform the data by selecting only three columns\ndroppedDF = (pagecountsEnAllDF\n  .drop(\"bytes_served\") # Our second transformation after the initial read (4 columns down to 3)\n)\n# Now let's take a look at what the schema looks like\ndroppedDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["Again, `drop(..)` is just one more transformation - that is no job is triggered."],"metadata":{}},{"cell_type":"code","source":["# And lastly, show the first five records which should exclude the bytes_served column.\ndroppedDF.show(5, False)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) distinct() & dropDuplicates()\n\nThese two transformations do the same thing. In fact, they are aliases for one another.\n* You can see this by looking at the source code for these two methods\n* ```def distinct(): Dataset[T] = dropDuplicates()```\n* See <a href=\"https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala\" target=\"_blank\">Dataset.scala</a>\n\nThe difference between them has everything to do with the programmer and their perspective.\n* The name **distinct** will resonate with developers, analyst and DB admins with a background in SQL.\n* The name **dropDuplicates** will resonate with developers that have a background or experience in functional programming.\n\nAs you become more familiar with the various APIs, you will see this pattern reassert itself.\n\nThe designers of the API are trying to make the API as approachable as possible for multiple target audiences.\n\nIf you look at the API docs, both `distinct(..)` and `dropDuplicates(..)` are described like this:\n> Returns a new Dataset that contains only the unique rows from this Dataset....\n\nWith this transformation, we can now tackle our first business question:"],"metadata":{}},{"cell_type":"markdown","source":["### How many different English Wikimedia projects saw traffic during that hour?"],"metadata":{}},{"cell_type":"markdown","source":["If you recall, our original `DataFrame` has this schema:"],"metadata":{}},{"cell_type":"code","source":["pagecountsEnAllDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["The transformation `distinct()` is applied to the row as a whole - data in the **project**, **article** and **requests** column will effect this evaluation.\n\nTo get the distinct list of projects, and only projects, we need to reduce the number of columns to just the one column, **project**. \n\nWe can do this with the `select(..)` transformation and then we can introduce the `distinct()` transformation."],"metadata":{}},{"cell_type":"code","source":["distinctDF = (pagecountsEnAllDF     # Our original DataFrame from spark.read.parquet(..)\n  .select(\"project\")                # Drop all columns except the \"project\" column\n  .distinct()                       # Reduce the set of all records to just the distinct column.\n)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["Just to reinforce, we have three transformations:\n0. Read the data (now represented by `pagecountsEnAllDF`)\n0. Select just the one column\n0. Reduce the records to a distinct set\n\nNo job is triggered until we perform an action like `show(..)`:"],"metadata":{}},{"cell_type":"code","source":["# There will not be more than 100 projects\ndistinctDF.show(100, False)               "],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["You can count those if you like.\n\nBut, it would be easier to ask the `DataFrame` for the `count()`:"],"metadata":{}},{"cell_type":"code","source":["total = distinctDF.count()     \nprint(\"Distinct Projects: {0:,}\".format( total ))"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) dropDuplicates(columns...)\n\nThe method `dropDuplicates(..)` has a second variant that accepts one or more columns.\n* The distinction is not performed across the entire record unlike `distinct()` or even `dropDuplicates()`.\n* The distinction is based only on the specified columns.\n* This allows us to keep all the original columns in our `DataFrame`."],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Recap\n\nOur code is spread out over many cells which can make this a little hard to follow.\n\nLet's take a look at the same code in a single cell."],"metadata":{}},{"cell_type":"code","source":["parquetDir = \"/mnt/training/wikipedia/pagecounts/staging_parquet_en_only_clean/\"\n\npagecountsEnAllDF = (spark       # Our SparkSession & Entry Point\n  .read                          # Our DataFrameReader\n  .parquet(parquetDir)           # Returns an instance of DataFrame\n)\n(pagecountsEnAllDF               # Only if we are running multiple queries\n  .cache()                       # mark the DataFrame as cachable\n  .count()                       # materialize the cache\n)\ndistinctDF = (pagecountsEnAllDF  # Our original DataFrame from spark.read.parquet(..)\n  .select(\"project\")             # Drop all columns except the \"project\" column\n  .distinct()                    # Reduce the set of all records to just the distinct column.\n)\ntotal = distinctDF.count()     \nprint(\"Distinct Projects: {0:,}\".format( total ))"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) DataFrames vs SQL & Temporary Views\n\n\nThis might also be a good time to read up on the history and difference between [RDDs, DataFrames, and Datasets](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html). \n\nThe `DataFrame`s API is built upon a SQL engine.\n\nAs such we can \"convert\" a `DataFrame` into a temporary view (or table) and then use it in \"standard\" SQL.\n\nLet's start by creating a temporary view from a previous `DataFrame`."],"metadata":{}},{"cell_type":"code","source":["pagecountsEnAllDF.createOrReplaceTempView(\"pagecounts\")"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["Now that we have a temporary view (or table) we can start expressing our queries and transformations in SQL:"],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT * FROM pagecounts"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["And we can just as easily express in SQL the distinct list of projects, and just because we can, we'll sort that list:"],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT DISTINCT project FROM pagecounts ORDER BY project"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":["And converting from SQL back to a `DataFrame` is just as easy:"],"metadata":{}},{"cell_type":"code","source":["tableDF = spark.sql(\"SELECT DISTINCT project FROM pagecounts ORDER BY project\")\ndisplay(tableDF)"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":["-sandbox\n&copy; 2018 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"02_data_prep_w_Spark","notebookId":4057188818416031},"nbformat":4,"nbformat_minor":0}