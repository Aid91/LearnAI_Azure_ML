{"cells":[{"cell_type":"markdown","source":["-sandbox\n# Machine Learning Pipeline\n\n** What you will learn:**\n* How to create a Machine Learning Pipeline.\n* How to train a Machine Learning model.\n* How to save & read the model.\n* How to make predictions with the model."],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"../includes/mnt_blob\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## The Data\n\nThe dataset contains bike rental info from 2011 and 2012 in the Capital bikeshare system, plus additional relevant information such as weather.  \n\nThis dataset is from Fanaee-T and Gama (2013) and is hosted by the <a href=\"http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\" target=\"_blank\">UCI Machine Learning Repository</a>."],"metadata":{}},{"cell_type":"markdown","source":["## The Goal\nWe want to learn to predict bike rental counts (per hour) from information such as day of the week, weather, month, etc.  \n\nHaving good predictions of customer demand allows a business or service to prepare and increase supply as needed."],"metadata":{}},{"cell_type":"markdown","source":["## Loading the data\n\nWe begin by loading our data, which is stored in the CSV format</a>."],"metadata":{}},{"cell_type":"code","source":["fileName = \"/mnt/data/bikeSharing/data-001/hour.csv\"\n\ninitialDF = (spark.read          # Our DataFrameReader\n  .option(\"header\", \"true\")      # Let Spark know we have a header\n  .option(\"inferSchema\", \"true\") # Infering the schema (it is a small dataset)\n  .csv(fileName)                 # Location of our data\n  .cache()                       # Mark the DataFrame as cached.\n)\n\ninitialDF.count()                # Materialize the cache\n\ninitialDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Understanding the data\n\nAccording to the <a href=\"http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\" target=\"_blank\">UCI ML Repository description</a>, we have the following schema:\n\n**Feature columns**:\n* **dteday**: date\n* **season**: season (1:spring, 2:summer, 3:fall, 4:winter)\n* **yr**: year (0:2011, 1:2012)\n* **mnth**: month (1 to 12)\n* **hr**: hour (0 to 23)\n* **holiday**: whether the day was a holiday or not\n* **weekday**: day of the week\n* **workingday**: `1` if the day is neither a weekend nor holiday, otherwise `0`.\n* **weathersit**: \n  * 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n  * 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n  * 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n  * 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n* **temp**: Normalized temperature in Celsius. The values are derived via `(t-t_min)/(t_max-t_min)`, `t_min=-8`, `t_max=+39` (only in hourly scale)\n* **atemp**: Normalized feeling temperature in Celsius. The values are derived via `(t-t_min)/(t_max-t_min)`, `t_min=-16`, `t_max=+50` (only in hourly scale)\n* **hum**: Normalized humidity. The values are divided to 100 (max)\n* **windspeed**: Normalized wind speed. The values are divided to 67 (max)\n\n**Label columns**:\n* **casual**: count of casual users\n* **registered**: count of registered users\n* **cnt**: count of total rental bikes including both casual and registered\n\n**Extraneous columns**:\n* **instant**: record index\n\nFor example, the first row is a record of hour 0 on January 1, 2011---and apparently, 16 people rented bikes around midnight!"],"metadata":{}},{"cell_type":"markdown","source":["## Preprocessing the data\n\nSo what do we need to do to get our data ready for Machine Learning?\n\n**Recall our goal**: We want to learn to predict the count of bike rentals (the `cnt` column).  We refer to the count as our target \"label\".\n\n**Features**: What can we use as features to predict the `cnt` label?  \n\nAll the columns except `cnt`, and a few exceptions:\n* `casual` & `registered`\n  * The `cnt` column we want to predict equals the sum of the `casual` + `registered` columns.  We will remove the `casual` and `registered` columns from the data to make sure we do not use them to predict `cnt`.  (*Warning: This is a danger in careless Machine Learning.  Make sure you do not \"cheat\" by using information you will not have when making predictions*)\n* `season` and the date column `dteday`: We could keep them, but they are well-represented by the other date-related columns like `yr`, `mnth`, and `weekday`.\n* `holiday` and `weekday`: These features are highly correlated with the `workingday` column.\n* row index column `instant`: This is a useless column to us."],"metadata":{}},{"cell_type":"markdown","source":["Let's drop the columns `instant`, `dteday`, `season`, `casual`, `holiday`, `weekday`, and `registered` from our DataFrame and then review our schema:"],"metadata":{}},{"cell_type":"code","source":["preprocessedDF = initialDF.drop(\"instant\", \"dteday\", \"season\", \"casual\", \"registered\", \"holiday\", \"weekday\")\n\npreprocessedDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Train/Test Split\n\nOur final data preparation step will be to split our dataset into separate training and test sets.\n\nUsing the `randomSplit()` function, we split the data such that 70% of the data is reserved for training and the remaining 30% for testing. \n\nFor more information see:\n* Scala: <a href=\"https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.Dataset\" target=\"_blank\">Dataset.randomSplit()</a>\n* Python: <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit\" target=\"_blank\">DataFrame.randomSplit()</a>"],"metadata":{}},{"cell_type":"code","source":["trainDF, testDF = preprocessedDF.randomSplit(\n  [0.7, 0.3],  # 70-30 split\n  seed=42)     # For reproducibility\n\nprint(\"We have %d training examples and %d test examples.\" % (trainDF.count(), testDF.count()))\nassert (trainDF.count() == 12197)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## Visualize our data\n\nNow that we have preprocessed our features, we can quickly visualize our data to get a sense of whether the features are meaningful.\n\nWe want to compare bike rental counts versus the hour of the day. \n\nTo plot the data:\n* Run the cell below\n* From the list of plot types, select **Line**.\n* Click the **Plot Options...** button.\n* By dragging and dropping the fields, set the **Keys** to **hr** and the **Values** to **cnt**.\n\nOnce you've created the graph, go back and select different **Keys**. For example:\n* **cnt** vs. **windspeed**\n* **cnt** vs. **month**\n* **cnt** vs. **workingday**\n* **cnt** vs. **hum**\n* **cnt** vs. **temp**\n* ...etc."],"metadata":{}},{"cell_type":"code","source":["display(trainDF)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["A couple of notes:\n* Rentals are low during the night, and they peak in the morning (8 am) and in the early evening (5 pm).  \n* Rentals are high during the summer and low in winter.\n* Rentals are high on working days vs. non-working days\n\nThis indicates that the `hr`, `mnth` and `workingday` features are all useful and can help us predict our label `cnt`. \n\nBut how do other features affect our prediction? \n\nDo combinations of those features matter? For example, high wind in summer is not going to have the same effect as high wind in winter.\n\nAs it turns out our features can be divided into two types:\n * **Numeric columns:**\n   * `mnth`\n   * `temp`\n   * `hr`\n   * `hum`\n   * `atemp`\n   * `windspeed`\n\n* **Categorical Columns:**\n  * `yr`\n  * `workingday`\n  * `weathersit`\n  \nWe could treat both `mnth` and `hr` as categorical but we would lose the temporal relationships (e.g. 2:00 AM comes before 3:00 AM)."],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) StringIndexer\n\nFor each of the categorical columns, we are going to create one `StringIndexer` where we\n  * Set `inputCol` to something like `weathersit`\n  * Set `outputCol` to something like `weathersitIndex`\n\nThis will have the effect of treating a value like `weathersit` not as number 1 through 4, but rather four categories: **light**, **mist**, **medium** & **heavy**, for example.\n\nFor more information see:\n* Scala: <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.StringIndexer\" target=\"_blank\">StringIndexer</a>\n* Python: <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=stringindexer#pyspark.ml.feature.StringIndexer\" target=\"_blank\">StringIndexer</a>"],"metadata":{}},{"cell_type":"markdown","source":["Before we get started, let's review our current schema:"],"metadata":{}},{"cell_type":"code","source":["trainDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Let's create the first `StringIndexer` for the `workingday` column.\n\nAfter we create it, we can run a sample through the indexer to see how it would affect our `DataFrame`."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\n\nworkingdayStringIndexer = StringIndexer(\n  inputCol=\"workingday\", \n  outputCol=\"workingdayIndex\")\n\n# Just for demonstration purposes, we will use the StringIndexer to fit and\n# then transform our training data set just to see how it affects the schema\nworkingdayStringIndexer.fit(trainDF).transform(trainDF).printSchema()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Next we will create the `StringIndexer` for the `yr` column and preview its effect."],"metadata":{}},{"cell_type":"code","source":["yrStringIndexer = StringIndexer(\n  inputCol=\"yr\", \n  outputCol=\"yrIndex\")\n\nyrStringIndexer.fit(trainDF).transform(trainDF).printSchema()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["And then create our last `StringIndexer` for the `weathersit` column."],"metadata":{}},{"cell_type":"code","source":["weathersitStringIndexer = StringIndexer(\n  inputCol=\"weathersit\", \n  outputCol=\"weathersitIndex\")\n\nweathersitStringIndexer.fit(trainDF).transform(trainDF).printSchema()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) VectorAssembler\n\nThe next step is to assemble the feature columns into a single feature vector.\n\nTo do that we will use the `VectorAssembler` where we\n  * Set `inputCols` to the new list of feature columns\n  * Set `outputCol` to `features`\n  \n  \nFor more information see:\n* Scala: <a href=\"https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.VectorAssembler\" target=\"_blank\">VectorAssembler</a>\n* Python: <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler\" target=\"_blank\">VectorAssembler</a>"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\nassemblerInputs  = [\n  \"mnth\", \"temp\", \"hr\", \"hum\", \"atemp\", \"windspeed\", # Our numerical features\n  \"yrIndex\", \"workingdayIndex\", \"weathersit\"]        # Our new categorical features\n\nvectorAssembler = VectorAssembler(\n  inputCols=assemblerInputs, \n  outputCol=\"features\")"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Random Forests\n\nRandom forests and ensembles of decision trees are more powerful than a single decision tree alone.\n\nThis is also the last step in our pipeline.\n\nWe will use the `RandomForestRegressor` where we\n  * Set `labelCol` to the column that contains our label.\n  * Set `seed` to ensure reproducibility.\n  * Set `numTrees` to `3` so that we build 3 trees in our random forest.\n  * Set `maxDepth` to `10` to control the depth/complexity of the tree.\n\nFor more information see:\n* Scala: <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.RandomForestRegressor\" target=\"_blank\">RandomForestRegressor</a>\n* Python: <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor\" target=\"_blank\">RandomForestRegressor</a>"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import RandomForestRegressor\n\nrfr = (RandomForestRegressor()\n      .setLabelCol(\"cnt\") # The column of our label\n      .setSeed(27)        # Some seed value for consistency\n      .setNumTrees(3)     # A guess at the number of trees\n      .setMaxDepth(10)    # A guess at the depth of each tree\n)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Create a Machine Learning Pipeline\n\nNow let's wrap all of these stages into a Pipeline."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\npipeline = Pipeline().setStages([\n  workingdayStringIndexer, # categorize workingday\n  weathersitStringIndexer, # categorize weathersit\n  yrStringIndexer,         # categorize yr\n  vectorAssembler,         # assemble the feature vector for all columns\n  rfr])"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Train the model\n\nTrain the pipeline model to run all the steps in the pipeline."],"metadata":{}},{"cell_type":"code","source":["pipelineModel = pipeline.fit(trainDF)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Evaluate the model\n\nNow that we have fitted a model, we can evaluate it.\n\nIn the case of a random forest, one of the best things to look at is the `featureImportances`:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import RandomForestRegressionModel\n\nrfrm = pipelineModel.stages[-1] # The RFRM is in the last stage of the model\n\n#  Zip the list of features with their scores\nscores = zip(assemblerInputs, rfrm.featureImportances)\n\n# And pretty print 'em\nfor x in scores: print(\"%-15s = %s\" % x)\n\nprint(\"-\"*80)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["Which features were most important?"],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Making Predictions\n\nNext, apply the trained pipeline model to the test set."],"metadata":{}},{"cell_type":"code","source":["# Using the model, create our predictions from the test data\npredictionsDF = pipelineModel.transform(testDF)\n\n# Reorder the columns for easier interpretation\nreorderedDF = predictionsDF.select(\"cnt\", \"prediction\", \"yr\", \"yrIndex\", \"mnth\", \"hr\", \"workingday\", \"workingdayIndex\", \"weathersit\", \"weathersitIndex\", \"temp\", \"atemp\", \"hum\", \"windspeed\")\n\ndisplay(reorderedDF)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Evaluate\n\nNext, we'll use `RegressionEvaluator` to assess the results. The default regression metric is RMSE.\n\nFor more information see:\n* Scala: <a href=\"https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.evaluation.RegressionEvaluator\" target=\"_blank\">RegressionEvaluator</a>\n* Python: <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator\" target=\"_blank\">RegressionEvaluator</a>"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n\nevaluator = RegressionEvaluator().setLabelCol(\"cnt\")\n\nrmse = evaluator.evaluate(predictionsDF)\n\nprint(\"Test RMSE = %f\" % rmse)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) ParamGrid\n\nThere are a lot of hyperparamaters we could tune, and it would take a long time to manually configure.\n\nInstead of a manual (ad-hoc) approach, let's use Spark's `ParamGridBuilder` to find the optimal hyperparameters in a more systematic approach.\n\nIn this example notebook, we keep these trees shallow and use a relatively small number of trees. Let's define a grid of hyperparameters to test:\n  - maxDepth: max depth of each decision tree in the RF ensemble (Use the values `2, 5, 10`)\n  - numTrees: number of trees in each RF ensemble (Use the values `10, 50`)\n\n`addGrid()` accepts the name of the parameter (e.g. `rf.maxDepth`), and an Array of the possible values (e.g. `Array(2, 5, 10)`).\n\nFor more information see:\n* Scala: <a href=\"https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.tuning.ParamGridBuilder\" target=\"_blank\">ParamGridBuilder</a>\n* Python: <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder\" target=\"_blank\">ParamGridBuilder</a>"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder\n\nparamGrid = (ParamGridBuilder()\n            .addGrid(rfr.maxDepth, [2, 5, 10])\n            .addGrid(rfr.numTrees, [10, 50])\n            .build())"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Cross-Validation\n\nWe are also going to use 3-fold cross-validation to identify the optimal maxDepth and numTrees combination.\n\n![crossValidation](https://files.training.databricks.com/images/301/CrossValidation.png)\n\nWith 3-fold cross-validation, we train on 2/3 of the data and evaluate with the remaining (held-out) 1/3. We repeat this process 3 times, so each fold gets the chance to act as the validation set. We then average the results of the three rounds."],"metadata":{}},{"cell_type":"markdown","source":["We pass in the `estimator` (our original pipeline), an `evaluator`, and an `estimatorParamMaps` to the `CrossValidator` so that it knows:\n- Which model to use\n- How to evaluate the model\n- What hyperparamters to set on the model\n\nWe can also set the number of folds we want to split our data into (3), as well as setting a seed so we all have the same split in the data.\n\nFor more information see:\n* Scala: <a href=\"https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.tuning.CrossValidator\" target=\"_blank\">CrossValidator</a>\n* Python: <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator\" target=\"_blank\">CrossValidator</a>"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.tuning import CrossValidator\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nevaluator = (RegressionEvaluator()\n  .setLabelCol(\"cnt\")\n  .setPredictionCol(\"prediction\"))\n\ncv = (CrossValidator()\n  .setEstimator(pipeline)\n  .setEvaluator(evaluator)\n  .setEstimatorParamMaps(paramGrid)\n  .setNumFolds(3)\n  .setSeed(27))"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) A New Model\n\nWe can now use the `CrossValidator` to fit a new model - this could take several minutes on a small cluster."],"metadata":{}},{"cell_type":"code","source":["cvModel = cv.fit(trainDF)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["And now we can take a look at the model with the best hyperparameter configuration:"],"metadata":{}},{"cell_type":"code","source":["# Zip the two lists together\nresults = list(zip(cvModel.getEstimatorParamMaps(), cvModel.avgMetrics))\n\n# And pretty print 'em\nfor x in results:\n  numTrees, rmse = list(x[0].values())\n  print(\"Depth: %s, Trees: %s\\nAverage: %s\\n\" % (numTrees, rmse, x[1]))\n  \nprint(\"-\"*80)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) One last set of predictions\n\nUsing our newest mode, let's make a final set of predictions:"],"metadata":{}},{"cell_type":"code","source":["# Using the model, create our predictions from the test data\nfinalPredictionsDF = cvModel.transform(testDF)\n\n# Reorder the columns for easier interpretation\nfinalDF = finalPredictionsDF.select(\"cnt\", \"prediction\", \"yr\", \"yrIndex\", \"mnth\", \"hr\", \"workingday\", \"workingdayIndex\", \"weathersit\", \"weathersitIndex\", \"temp\", \"atemp\", \"hum\", \"windspeed\")\n\ndisplay(finalDF)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Evaluating the New Model\n\nLet's see how our latest model does:"],"metadata":{}},{"cell_type":"code","source":["print(\"Test RMSE = %f\" % evaluator.evaluate(finalPredictionsDF))"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["-sandbox\n&copy; 2018 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"06_pipeline_demo","notebookId":3771102850753868},"nbformat":4,"nbformat_minor":0}
