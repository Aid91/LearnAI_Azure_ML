{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-sandbox\n",
    "# Machine Learning Pipeline\n",
    "\n",
    "** What you will learn:**\n",
    "* How to create a Machine Learning Pipeline.\n",
    "* How to train a Machine Learning model.\n",
    "* How to save & read the model.\n",
    "* How to make predictions with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../includes/mnt_blob\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The dataset contains bike rental info from 2011 and 2012 in the Capital bikeshare system, plus additional relevant information such as weather.  \n",
    "\n",
    "This dataset is from Fanaee-T and Gama (2013) and is hosted by the <a href=\"http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\" target=\"_blank\">UCI Machine Learning Repository</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Goal\n",
    "We want to learn to predict bike rental counts (per hour) from information such as day of the week, weather, month, etc.  \n",
    "\n",
    "Having good predictions of customer demand allows a business or service to prepare and increase supply as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "We begin by loading our data, which is stored in the CSV format</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = \"/mnt/data/bikeSharing/data-001/hour.csv\"\n",
    "\n",
    "initialDF = (spark.read          # Our DataFrameReader\n",
    "  .option(\"header\", \"true\")      # Let Spark know we have a header\n",
    "  .option(\"inferSchema\", \"true\") # Infering the schema (it is a small dataset)\n",
    "  .csv(fileName)                 # Location of our data\n",
    "  .cache()                       # Mark the DataFrame as cached.\n",
    ")\n",
    "\n",
    "initialDF.count()                # Materialize the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the data\n",
    "\n",
    "According to the <a href=\"http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\" target=\"_blank\">UCI ML Repository description</a>, we have the following schema:\n",
    "\n",
    "**Feature columns**:\n",
    "* **dteday**: date\n",
    "* **season**: season (1:spring, 2:summer, 3:fall, 4:winter)\n",
    "* **yr**: year (0:2011, 1:2012)\n",
    "* **mnth**: month (1 to 12)\n",
    "* **hr**: hour (0 to 23)\n",
    "* **holiday**: whether the day was a holiday or not\n",
    "* **weekday**: day of the week\n",
    "* **workingday**: `1` if the day is neither a weekend nor holiday, otherwise `0`.\n",
    "* **weathersit**: \n",
    "  * 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "  * 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "  * 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "  * 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "* **temp**: Normalized temperature in Celsius. The values are derived via `(t-t_min)/(t_max-t_min)`, `t_min=-8`, `t_max=+39` (only in hourly scale)\n",
    "* **atemp**: Normalized feeling temperature in Celsius. The values are derived via `(t-t_min)/(t_max-t_min)`, `t_min=-16`, `t_max=+50` (only in hourly scale)\n",
    "* **hum**: Normalized humidity. The values are divided to 100 (max)\n",
    "* **windspeed**: Normalized wind speed. The values are divided to 67 (max)\n",
    "\n",
    "**Label columns**:\n",
    "* **casual**: count of casual users\n",
    "* **registered**: count of registered users\n",
    "* **cnt**: count of total rental bikes including both casual and registered\n",
    "\n",
    "**Extraneous columns**:\n",
    "* **instant**: record index\n",
    "\n",
    "For example, the first row is a record of hour 0 on January 1, 2011---and apparently, 16 people rented bikes around midnight!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "So what do we need to do to get our data ready for Machine Learning?\n",
    "\n",
    "**Recall our goal**: We want to learn to predict the count of bike rentals (the `cnt` column).  We refer to the count as our target \"label\".\n",
    "\n",
    "**Features**: What can we use as features to predict the `cnt` label?  \n",
    "\n",
    "All the columns except `cnt`, and a few exceptions:\n",
    "* `casual` & `registered`\n",
    "  * The `cnt` column we want to predict equals the sum of the `casual` + `registered` columns.  We will remove the `casual` and `registered` columns from the data to make sure we do not use them to predict `cnt`.  (*Warning: This is a danger in careless Machine Learning.  Make sure you do not \"cheat\" by using information you will not have when making predictions*)\n",
    "* `season` and the date column `dteday`: We could keep them, but they are well-represented by the other date-related columns like `yr`, `mnth`, and `weekday`.\n",
    "* `holiday` and `weekday`: These features are highly correlated with the `workingday` column.\n",
    "* row index column `instant`: This is a useless column to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the feature columns `instant`, `dteday`, `season`, `holiday`, `weekday`, and label columns `casual` and `registered` from our DataFrame and then review our schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessedDF = initialDF.drop(\"instant\", \"dteday\", \"season\", \"holiday\", \"weekday\", \"casual\", \"registered\")\n",
    "\n",
    "preprocessedDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Train/Test Split\n",
    "\n",
    "Our final data preparation step will be to split our dataset into separate training and test sets.\n",
    "\n",
    "Using the `randomSplit()` function, we split the data such that 70% of the data is reserved for training and the remaining 30% for testing. \n",
    "\n",
    "For more information see:\n",
    "* Scala: <a href=\"https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.Dataset\" target=\"_blank\">Dataset.randomSplit()</a>\n",
    "* Python: <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit\" target=\"_blank\">DataFrame.randomSplit()</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF, testDF = preprocessedDF.randomSplit(\n",
    "  [0.7, 0.3],  # 70-30 split\n",
    "  seed=42)     # For reproducibility\n",
    "\n",
    "print(\"We have %d training examples and %d test examples.\" % (trainDF.count(), testDF.count()))\n",
    "assert (trainDF.count() == 12197)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize our data\n",
    "\n",
    "Now that we have preprocessed our features, we can quickly visualize our data to get a sense of whether the features are meaningful.\n",
    "\n",
    "We want to compare bike rental counts versus the hour of the day. \n",
    "\n",
    "To plot the data:\n",
    "* Run the cell below\n",
    "* From the list of plot types, select **Line**.\n",
    "* Click the **Plot Options...** button.\n",
    "* By dragging and dropping the fields, set the **Keys** to **hr** and the **Values** to **cnt**.\n",
    "\n",
    "Once you've created the graph, go back and select different **Keys**. For example:\n",
    "* **cnt** vs. **windspeed**\n",
    "* **cnt** vs. **month**\n",
    "* **cnt** vs. **workingday**\n",
    "* **cnt** vs. **hum**\n",
    "* **cnt** vs. **temp**\n",
    "* ...etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trainDF) #.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of notes:\n",
    "* Rentals are low during the night, and they peak in the morning (8 am) and in the early evening (5 pm).  \n",
    "* Rentals are high during the summer and low in winter.\n",
    "* Rentals are high on working days vs. non-working days\n",
    "\n",
    "This indicates that the `hr`, `mnth` and `workingday` features are all useful and can help us predict our label `cnt`. \n",
    "\n",
    "But how do other features affect our prediction? \n",
    "\n",
    "Do combinations of those features matter? For example, high wind in summer is not going to have the same effect as high wind in winter.\n",
    "\n",
    "As it turns out our features can be divided into two types:\n",
    " * **Numeric columns:**\n",
    "   * `mnth`\n",
    "   * `temp`\n",
    "   * `hr`\n",
    "   * `hum`\n",
    "   * `atemp`\n",
    "   * `windspeed`\n",
    "\n",
    "* **Categorical Columns:**\n",
    "  * `yr`\n",
    "  * `workingday`\n",
    "  * `weathersit`\n",
    "  \n",
    "We could treat both `mnth` and `hr` as categorical but we would lose the temporal relationships (e.g. 2:00 AM comes before 3:00 AM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) StringIndexer\n",
    "\n",
    "For each of the categorical columns, we are going to create one `StringIndexer` where we\n",
    "  * Set `inputCol` to something like `weathersit`\n",
    "  * Set `outputCol` to something like `weathersitIndex`\n",
    "\n",
    "This will have the effect of treating a value like `weathersit` not as number 1 through 4, but rather four categories: **light**, **mist**, **medium** & **heavy**, for example.\n",
    "\n",
    "For more information see:\n",
    "* Scala: <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.StringIndexer\" target=\"_blank\">StringIndexer</a>\n",
    "* Python: <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=stringindexer#pyspark.ml.feature.StringIndexer\" target=\"_blank\">StringIndexer</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, let's review our current schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the first `StringIndexer` for the `workingday` column.\n",
    "\n",
    "After we create it, we can run a sample through the indexer to see how it would affect our `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "workingdayStringIndexer = StringIndexer(\n",
    "  inputCol=\"workingday\", \n",
    "  outputCol=\"workingdayIndex\")\n",
    "\n",
    "# Just for demonstration purposes, we will use the StringIndexer to fit and\n",
    "# then transform our training data set just to see how it affects the schema\n",
    "workingdayStringIndexer.fit(trainDF).transform(trainDF).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the `StringIndexer` for the `yr` column and preview its effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yrStringIndexer = StringIndexer(\n",
    "  inputCol=\"yr\", \n",
    "  outputCol=\"yrIndex\")\n",
    "\n",
    "yrStringIndexer.fit(trainDF).transform(trainDF).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then create our last `StringIndexer` for the `weathersit` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weathersitStringIndexer = StringIndexer(\n",
    "  inputCol=\"weathersit\", \n",
    "  outputCol=\"weathersitIndex\")\n",
    "\n",
    "weathersitStringIndexer.fit(trainDF).transform(trainDF).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) VectorAssembler\n",
    "\n",
    "The next step is to assemble the feature columns into a single feature vector.\n",
    "\n",
    "To do that we will use the `VectorAssembler` where we\n",
    "  * Set `inputCols` to the new list of feature columns\n",
    "  * Set `outputCol` to `features`\n",
    "  \n",
    "  \n",
    "For more information see:\n",
    "* Scala: <a href=\"https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.VectorAssembler\" target=\"_blank\">VectorAssembler</a>\n",
    "* Python: <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler\" target=\"_blank\">VectorAssembler</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assemblerInputs  = [\n",
    "  \"mnth\", \"temp\", \"hr\", \"hum\", \"atemp\", \"windspeed\", # Our numerical features\n",
    "  \"yrIndex\", \"workingdayIndex\", \"weathersit\"]        # Our new categorical features\n",
    "\n",
    "vectorAssembler = VectorAssembler(\n",
    "  inputCols=assemblerInputs, \n",
    "  outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Random Forests\n",
    "\n",
    "Random forests and ensembles of decision trees are more powerful than a single decision tree alone.\n",
    "\n",
    "This is also the last step in our pipeline.\n",
    "\n",
    "We will use the `RandomForestRegressor` where we\n",
    "  * Set `labelCol` to the column that contains our label.\n",
    "  * Set `seed` to ensure reproducibility.\n",
    "  * Set `numTrees` to `3` so that we build 3 trees in our random forest.\n",
    "  * Set `maxDepth` to `10` to control the depth/complexity of the tree.\n",
    "\n",
    "For more information see:\n",
    "* Scala: <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.RandomForestRegressor\" target=\"_blank\">RandomForestRegressor</a>\n",
    "* Python: <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor\" target=\"_blank\">RandomForestRegressor</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rfr = (RandomForestRegressor()\n",
    "      .setLabelCol(\"cnt\") # The column of our label\n",
    "#      .setFeatureCols(\"features\")\n",
    "      .setSeed(27)        # Some seed value for consistency\n",
    "      .setNumTrees(3)     # A guess at the number of trees\n",
    "      .setMaxDepth(10)    # A guess at the depth of each tree\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Create a Machine Learning Pipeline\n",
    "\n",
    "Now let's wrap all of these stages into a Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "  workingdayStringIndexer, # categorize workingday\n",
    "  weathersitStringIndexer, # categorize weathersit\n",
    "  yrStringIndexer,         # categorize yr\n",
    "  vectorAssembler,         # assemble the feature vector for all columns\n",
    "  rfr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Train the model\n",
    "\n",
    "Train the pipeline model to run all the steps in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Evaluate the model\n",
    "\n",
    "Now that we have fitted a model, we can evaluate it.\n",
    "\n",
    "In the case of a random forest, one of the best things to look at is the `featureImportances`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "rfrm = pipelineModel.stages[-1] # The RFRM is in the last stage of the model\n",
    "\n",
    "#  Zip the list of features with their scores\n",
    "scores = zip(assemblerInputs, rfrm.featureImportances)\n",
    "\n",
    "# And pretty print 'em\n",
    "for x in scores: print(\"%-15s = %s\" % x)\n",
    "\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features were most important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Making Predictions\n",
    "\n",
    "Next, apply the trained pipeline model to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the model, create our predictions from the test data\n",
    "predictionsDF = pipelineModel.transform(testDF)\n",
    "\n",
    "# Reorder the columns for easier interpretation\n",
    "reorderedDF = predictionsDF.select(\"cnt\", \"prediction\", \"yr\", \"yrIndex\", \"mnth\", \"hr\", \"workingday\", \"workingdayIndex\", \"weathersit\", \"weathersitIndex\", \"temp\", \"atemp\", \"hum\", \"windspeed\")\n",
    "\n",
    "display(reorderedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Evaluate\n",
    "\n",
    "Next, we'll use `RegressionEvaluator` to assess the results. The default regression metric is RMSE.\n",
    "\n",
    "For more information see:\n",
    "* Scala: <a href=\"https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.evaluation.RegressionEvaluator\" target=\"_blank\">RegressionEvaluator</a>\n",
    "* Python: <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator\" target=\"_blank\">RegressionEvaluator</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator().setLabelCol(\"cnt\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictionsDF)\n",
    "\n",
    "print(\"Test RMSE = %f\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) ParamGrid\n",
    "\n",
    "There are a lot of hyperparamaters we could tune, and it would take a long time to manually configure.\n",
    "\n",
    "Instead of a manual (ad-hoc) approach, let's use Spark's `ParamGridBuilder` to find the optimal hyperparameters in a more systematic approach.\n",
    "\n",
    "In this example notebook, we keep these trees shallow and use a relatively small number of trees. Let's define a grid of hyperparameters to test:\n",
    "  - maxDepth: max depth of each decision tree in the RF ensemble (Use the values `2, 5, 10`)\n",
    "  - numTrees: number of trees in each RF ensemble (Use the values `10, 50`)\n",
    "\n",
    "`addGrid()` accepts the name of the parameter (e.g. `rf.maxDepth`), and an Array of the possible values (e.g. `Array(2, 5, 10)`).\n",
    "\n",
    "For more information see:\n",
    "* Scala: <a href=\"https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.tuning.ParamGridBuilder\" target=\"_blank\">ParamGridBuilder</a>\n",
    "* Python: <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder\" target=\"_blank\">ParamGridBuilder</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(rfr.maxDepth, [2, 5, 10])\n",
    "            .addGrid(rfr.numTrees, [10, 50])\n",
    "            .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Cross-Validation\n",
    "\n",
    "We are also going to use 3-fold cross-validation to identify the optimal maxDepth and numTrees combination.\n",
    "\n",
    "![crossValidation](https://files.training.databricks.com/images/301/CrossValidation.png)\n",
    "\n",
    "With 3-fold cross-validation, we train on 2/3 of the data and evaluate with the remaining (held-out) 1/3. We repeat this process 3 times, so each fold gets the chance to act as the validation set. We then average the results of the three rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass in the `estimator` (our original pipeline), an `evaluator`, and an `estimatorParamMaps` to the `CrossValidator` so that it knows:\n",
    "- Which model to use\n",
    "- How to evaluate the model\n",
    "- What hyperparamters to set on the model\n",
    "\n",
    "We can also set the number of folds we want to split our data into (3), as well as setting a seed so we all have the same split in the data.\n",
    "\n",
    "For more information see:\n",
    "* Scala: <a href=\"https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.tuning.CrossValidator\" target=\"_blank\">CrossValidator</a>\n",
    "* Python: <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator\" target=\"_blank\">CrossValidator</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = (RegressionEvaluator()\n",
    "  .setLabelCol(\"cnt\")\n",
    "  .setPredictionCol(\"prediction\"))\n",
    "\n",
    "cv = (CrossValidator()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(evaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(3)\n",
    "  .setSeed(27))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) A New Model\n",
    "\n",
    "We can now use the `CrossValidator` to fit a new model - this could take several minutes on a small cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can take a look at the model with the best hyperparameter configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the two lists together\n",
    "results = list(zip(cvModel.getEstimatorParamMaps(), cvModel.avgMetrics))\n",
    "\n",
    "# And pretty print 'em\n",
    "for x in results:\n",
    "  numTrees, rmse = list(x[0].values())\n",
    "  print(\"Depth: %s, Trees: %s\\nAverage: %s\\n\" % (numTrees, rmse, x[1]))\n",
    "  \n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) One last set of predictions\n",
    "\n",
    "Using our newest mode, let's make a final set of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the model, create our predictions from the test data\n",
    "finalPredictionsDF = cvModel.transform(testDF)\n",
    "\n",
    "# Reorder the columns for easier interpretation\n",
    "finalDF = finalPredictionsDF.select(\"cnt\", \"prediction\", \"yr\", \"yrIndex\", \"mnth\", \"hr\", \"workingday\", \"workingdayIndex\", \"weathersit\", \"weathersitIndex\", \"temp\", \"atemp\", \"hum\", \"windspeed\")\n",
    "\n",
    "display(finalDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Evaluating the New Model\n",
    "\n",
    "Let's see how our latest model does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test RMSE = %f\" % evaluator.evaluate(finalPredictionsDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-sandbox\n",
    "&copy; 2018 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "name": "06_pipeline_demo",
  "notebookId": 4057188818415972
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
